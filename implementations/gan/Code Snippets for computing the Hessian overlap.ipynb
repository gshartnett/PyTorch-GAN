{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the norm of the gradient\n",
    "#g_grad_norm = Variable(Tensor(1).fill_(0.0), requires_grad=True)\n",
    "#for p in generator.parameters():\n",
    "#    g_grad_norm += p.grad.data.norm(2).item()**2\n",
    "#g_grad_norm = g_grad_norm **(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## look at 1d measures of UM\n",
    "## look at raw MNIST data\n",
    "## double check code\n",
    "## look into top-subspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to take the \"Hessian vector product\", i.e. $ \\sum_j H_{ij} v_j $ for $H_{ij} = \\partial_i \\partial_j f$ and $v_j$ an arbitrary vector. \n",
    "\n",
    "This came from this PyTorch help forum post: https://discuss.pytorch.org/t/calculating-hessian-vector-product/11240/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## first, let v be an arbitrary vector:\n",
    "v = Variable(torch.Tensor([1, 1]))\n",
    "x = Variable(torch.Tensor([1, 1]), requires_grad=True)\n",
    "f = 3*x[0]**2 + 4*x[0]*x[1] + x[1]**2\n",
    "grad_f, = torch.autograd.grad(f, x, create_graph=True)\n",
    "z = grad_f @ v\n",
    "z.backward()\n",
    "print(x.grad)\n",
    "\n",
    "## now, let v be x - note that now the answer changes because the gradient also hits v\n",
    "x = Variable(torch.Tensor([1, 1]), requires_grad=True)\n",
    "v = x\n",
    "f = 3*x[0]**2 + 4*x[0]*x[1] + x[1]**2\n",
    "grad_f, = torch.autograd.grad(f, x, create_graph=True)\n",
    "z = grad_f @ v\n",
    "z.backward()\n",
    "print(x.grad)\n",
    "\n",
    "## lastly, clone + detach v so that the derivative does not hit it, even though it is related to x\n",
    "x = Variable(torch.Tensor([1, 1]), requires_grad=True)\n",
    "v = x.clone().detach()\n",
    "f = 3*x[0]**2 + 4*x[0]*x[1] + x[1]**2\n",
    "grad_f, = torch.autograd.grad(f, x, create_graph=True)\n",
    "z = grad_f @ v\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lastly, clone + detach v so that the derivative does not hit it, even though it is related to x\n",
    "x = Variable(torch.Tensor([1, 1]), requires_grad=True)\n",
    "f = 3*x[0]**2 + 4*x[0]*x[1] + x[1]**2\n",
    "grad_f, = torch.autograd.grad(f, x, create_graph=True)\n",
    "v = grad_f.clone().detach()\n",
    "z = grad_f @ v\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n",
    "gen_imgs = generator(z)\n",
    "g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "\n",
    "theta_g_tmp = []\n",
    "for param in generator.parameters():\n",
    "    theta_g_tmp.append(param.view(-1))\n",
    "theta_g = torch.cat(theta_g_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grad_g  = torch.autograd.grad(g_loss, generator.parameters(), create_graph=True)\n",
    "grad_g  = torch.autograd.grad(g_loss, theta_g, create_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "theta_g_tmp = []\n",
    "for param in generator.parameters():\n",
    "    theta_g_tmp.append(param.view(-1))\n",
    "theta_g = torch.cat(theta_g_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = grad_g @ theta_g\n",
    "z.backward()\n",
    "print(theta_g.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(10, 20)\n",
    "input = torch.randn(1, 10)\n",
    "out = linear(input).sum()\n",
    "grads = torch.autograd.grad([out], linear.parameters(), create_graph=True)\n",
    "flatten = torch.cat([g.reshape(-1) for g in grads if g is not None])\n",
    "x = torch.randn_like(flatten)\n",
    "print(flatten.shape)\n",
    "flatten2 = Variable(flatten.data, requires_grad=True)\n",
    "hvps = torch.autograd.grad([flatten2 @ x], linear.parameters(), allow_unused=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn_like(flatten)\n",
    "print(flatten.shape) ## torch.Size([1792])\n",
    "x2 = Variable(x.data, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvps = torch.autograd.grad([flatten @ x2], conv.parameters(), allow_unused=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hvps[1]) ## None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten2 = torch.cat([g.reshape(-1) for g in hvps if g is not None])\n",
    "print(flatten2.shape) ## torch.Size([1728])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## a simple neural network\n",
    "linear = nn.Linear(10, 20)\n",
    "x = torch.randn(1, 10)\n",
    "y = linear(x).sum()\n",
    "\n",
    "## compute the gradient and make a copy that is detached from the graph\n",
    "grad = torch.autograd.grad(y, linear.parameters(), create_graph=True)\n",
    "v = grad.clone().detach()\n",
    "\n",
    "## compute the Hessian vector product\n",
    "z = grad @ v\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lastly, clone + detach v so that the derivative does not hit it, even though it is related to x\n",
    "x = Variable(torch.Tensor([1, 1]), requires_grad=True)\n",
    "v = x.clone().detach()\n",
    "f = 3*x[0]**2 + 4*x[0]*x[1] + x[1]**2\n",
    "grad_f, = torch.autograd.grad(f, x, create_graph=True)\n",
    "z = grad_f @ v\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad.view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvps = torch.autograd.grad([flatten2 @ x], linear.parameters(), allow_unused=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = torch.cat([g.reshape(-1) for g in grads if g is not None])\n",
    "x = torch.randn_like(flatten)\n",
    "print(flatten.shape)\n",
    "flatten2 = Variable(flatten.data, requires_grad=True)\n",
    "hvps = torch.autograd.grad([flatten2 @ x], linear.parameters(), allow_unused=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lastly, clone + detach v so that the derivative does not hit it, even though it is related to x\n",
    "x = Variable(torch.Tensor([1, 1]), requires_grad=True)\n",
    "v = x.clone().detach()\n",
    "f = 3*x[0]**2 + 4*x[0]*x[1] + x[1]**2\n",
    "grad_f, = torch.autograd.grad(f, x, create_graph=True)\n",
    "z = grad_f @ v\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_f @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = Variable(torch.zeros(220), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(10, 20)\n",
    "x = torch.randn(1, 10)\n",
    "L = linear(x).sum()\n",
    "\n",
    "for p in linear.parameters():\n",
    "    theta = torch.zeros(p.size(), requires_grad=True)\n",
    "    theta += p.data\n",
    "    \n",
    "grads = torch.autograd.grad(L, theta, create_graph=True)\n",
    "\n",
    "#gnorm = Variable(torch.zeros(1), requires_grad=True)[0].sum()\n",
    "#for g in grads:\n",
    "#    gnorm = gnorm + g.pow(2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = torch.tensor(theta, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.grad(grads, theta, create_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "linear = torch.nn.Linear(10, 20)\n",
    "x = torch.randn(1, 10)\n",
    "L = linear(x).sum()**2\n",
    "grad = torch.autograd.grad(L, linear.parameters(), create_graph=True)\n",
    "gnorm = 0\n",
    "for g in grad:\n",
    "    gnorm = gnorm + g.pow(2).sum()\n",
    "grad2 = torch.autograd.grad(gnorm, linear.parameters(), create_graph=True)\n",
    "grad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PyTorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
