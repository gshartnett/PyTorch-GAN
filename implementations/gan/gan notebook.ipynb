{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adapted from the GAN implementation in the PyTorch-GAN model zoo:\n",
    "## https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_style(\"white\")\n",
    "%matplotlib inline\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('images', exist_ok=True)\n",
    "\n",
    "n_epochs = 200 #'number of epochs of training'\n",
    "batch_size = 64 #'size of the batches'\n",
    "lr = 0.0002 #'adam: learning rate'\n",
    "b1 = 0.5 #'adam: decay of first order momentum of gradient'\n",
    "b2 = 0.999 #'adam: decay of first order momentum of gradient'\n",
    "n_cpu = 4 #'number of cpu threads to use during batch generation'\n",
    "latent_dim = 100 #'dimensionality of the latent space'\n",
    "img_size = 28 #'size of each image dimension'\n",
    "channels = 1 #'number of image channels'\n",
    "sample_interval=400 #'interval betwen image samples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (channels, img_size, img_size)\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(latent_dim, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), *img_shape)\n",
    "        return img\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        validity = self.model(img_flat)\n",
    "\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()\n",
    "\n",
    "# Configure data loader\n",
    "os.makedirs('../../data/mnist', exist_ok=True)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../../data/mnist', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## useful functions for collecting triangle statistics\n",
    "\n",
    "def triplet_sample(X):\n",
    "    \"\"\"\n",
    "    given an array of images X, return a triplet\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        [i,j,k] = list(np.random.randint(0, X.shape[0], 3))\n",
    "        if len({i,j,k}) == 3:\n",
    "            return [X[i], X[j], X[k]]\n",
    "        \n",
    "        \n",
    "def distance_L2(x,y):\n",
    "    \"\"\"\n",
    "    given 2 images, x, y, return the normalized l2 distance between them,\n",
    "    normalized so that the max distance is 1\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum((x - y)**2))/56\n",
    "\n",
    "\n",
    "def distance_H(x,y):\n",
    "    \"\"\"\n",
    "    given 2 images, x, y, return the normalized Hamming distance between them,\n",
    "    \"\"\"\n",
    "    x_binary = np.asarray(x.reshape(784)>0)\n",
    "    y_binary = np.asarray(y.reshape(784)>0) \n",
    "    return np.count_nonzero(x_binary != y_binary)/len(x_binary)\n",
    "\n",
    "\n",
    "def triangle_distances_l2(x,y,z):\n",
    "    \"\"\"\n",
    "    return the sorted l2 distances of the triangle formed by the points (x,y,z)\n",
    "    \"\"\"\n",
    "    return sorted([distance_L2(x,y), distance_L2(x,z), distance_L2(y,z)])\n",
    "\n",
    "\n",
    "def triangle_distances_H(x,y,z):\n",
    "    \"\"\"\n",
    "    return the sorted normalized Hamming distances of the triangle formed by the points (x,y,z)\n",
    "    \"\"\"\n",
    "    return sorted([distance_H(x,y), distance_H(x,z), distance_H(y,z)])\n",
    "\n",
    "\n",
    "def angles(dxy, dxz, dyz):\n",
    "    \"\"\"\n",
    "    Given the 3 distances of a triangle, return the sorted angles\n",
    "    \"\"\"\n",
    "    theta_xy = np.arccos((-dxy**2 + dxz**2 + dyz**2)/(2*dxz*dyz))\n",
    "    theta_xz = np.arccos((dxy**2 - dxz**2 + dyz**2)/(2*dxy*dyz))\n",
    "    theta_yz = np.arccos((dxy**2 + dxz**2 - dyz**2)/(2*dxy*dxz))\n",
    "    return sorted([theta_xy, theta_xz, theta_yz])\n",
    "\n",
    "\n",
    "def triangle_distributions(X, Num):\n",
    "    \"\"\"\n",
    "    Given an array of samples 4, generate two arrays of shape (Num, 2).\n",
    "    \n",
    "    Array 1: the 2 dimensions are [dmid-dmin, dmax-dmid] (l2 distance)\n",
    "    Array 2: the 2 dimensions are [theta_min/theta_max, theta_min/theta_mid] (l2 distance)\n",
    "    Array 3: the 2 dimensions are [dmid-dmin, dmax-dmid] (Hamming distance)\n",
    "    Array 4: the 2 dimensions are [theta_min/theta_max, theta_min/theta_mid] (Hamming distance)\n",
    "    \"\"\"\n",
    "\n",
    "    K_distances_l2 = np.zeros((Num,2))\n",
    "    K_angles_l2 = np.zeros((Num,2))\n",
    "    K_distances_H = np.zeros((Num,2))\n",
    "    K_angles_H = np.zeros((Num,2))\n",
    "    \n",
    "    for i in range(Num):\n",
    "        [x, y, z] = triplet_sample(X)\n",
    "        \n",
    "        ## l2 distances\n",
    "        [d_min, d_mid, d_max] = triangle_distances_l2(x,y,z)\n",
    "        [theta_min, theta_mid, theta_max] = angles(d_min, d_mid, d_max)        \n",
    "\n",
    "        K_distances_l2[i,0] = d_mid - d_min\n",
    "        K_distances_l2[i,1] = d_max - d_mid        \n",
    "        K_angles_l2[i,0] = theta_min/theta_mid\n",
    "        K_angles_l2[i,1] = theta_min/theta_max\n",
    "\n",
    "        ## Hamming distances\n",
    "        [d_min, d_mid, d_max] = triangle_distances_l2(x,y,z)\n",
    "        [theta_min, theta_mid, theta_max] = angles(d_min, d_mid, d_max)        \n",
    "\n",
    "        K_distances_H[i,0] = d_mid - d_min\n",
    "        K_distances_H[i,1] = d_max - d_mid        \n",
    "        K_angles_H[i,0] = theta_min/theta_mid\n",
    "        K_angles_H[i,1] = theta_min/theta_max\n",
    "\n",
    "        \n",
    "    return [K_distances_l2, K_angles_l2, K_distances_H, K_angles_H]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 0/938] [D loss: 0.702290] [G loss: 0.690400]\n",
      "[Epoch 0/200] [Batch 400/938] [D loss: 0.371219] [G loss: 0.740968]\n",
      "[Epoch 0/200] [Batch 800/938] [D loss: 0.232573] [G loss: 2.164175]\n",
      "[Epoch 1/200] [Batch 262/938] [D loss: 0.472385] [G loss: 3.074948]\n",
      "[Epoch 1/200] [Batch 662/938] [D loss: 0.235233] [G loss: 1.870987]\n",
      "[Epoch 2/200] [Batch 124/938] [D loss: 0.704411] [G loss: 5.161714]\n",
      "[Epoch 2/200] [Batch 524/938] [D loss: 0.168405] [G loss: 2.434054]\n",
      "[Epoch 2/200] [Batch 924/938] [D loss: 0.174887] [G loss: 1.849967]\n",
      "[Epoch 3/200] [Batch 386/938] [D loss: 0.261089] [G loss: 1.197292]\n",
      "[Epoch 3/200] [Batch 786/938] [D loss: 0.262792] [G loss: 2.042330]\n",
      "[Epoch 4/200] [Batch 248/938] [D loss: 0.114492] [G loss: 3.086748]\n",
      "[Epoch 4/200] [Batch 648/938] [D loss: 0.260728] [G loss: 2.599126]\n",
      "[Epoch 5/200] [Batch 110/938] [D loss: 0.187764] [G loss: 1.882069]\n",
      "[Epoch 5/200] [Batch 510/938] [D loss: 0.332962] [G loss: 1.148920]\n",
      "[Epoch 5/200] [Batch 910/938] [D loss: 0.726651] [G loss: 5.127366]\n",
      "[Epoch 6/200] [Batch 372/938] [D loss: 0.193299] [G loss: 1.961660]\n",
      "[Epoch 6/200] [Batch 772/938] [D loss: 0.148293] [G loss: 1.817064]\n",
      "[Epoch 7/200] [Batch 234/938] [D loss: 0.202509] [G loss: 1.616586]\n",
      "[Epoch 7/200] [Batch 634/938] [D loss: 0.482927] [G loss: 4.830579]\n",
      "[Epoch 8/200] [Batch 96/938] [D loss: 0.200113] [G loss: 1.695527]\n",
      "[Epoch 8/200] [Batch 496/938] [D loss: 0.225471] [G loss: 2.090983]\n",
      "[Epoch 8/200] [Batch 896/938] [D loss: 0.174559] [G loss: 4.409604]\n",
      "[Epoch 9/200] [Batch 358/938] [D loss: 0.370681] [G loss: 0.865343]\n",
      "[Epoch 9/200] [Batch 758/938] [D loss: 0.294468] [G loss: 1.107038]\n",
      "[Epoch 10/200] [Batch 220/938] [D loss: 0.293807] [G loss: 3.333595]\n",
      "[Epoch 10/200] [Batch 620/938] [D loss: 0.176177] [G loss: 2.282201]\n",
      "[Epoch 11/200] [Batch 82/938] [D loss: 0.199770] [G loss: 2.719744]\n",
      "[Epoch 11/200] [Batch 482/938] [D loss: 0.428581] [G loss: 0.892689]\n",
      "[Epoch 11/200] [Batch 882/938] [D loss: 0.190743] [G loss: 2.386656]\n",
      "[Epoch 12/200] [Batch 344/938] [D loss: 0.184673] [G loss: 2.254759]\n",
      "[Epoch 12/200] [Batch 744/938] [D loss: 0.129504] [G loss: 2.887751]\n",
      "[Epoch 13/200] [Batch 206/938] [D loss: 0.125005] [G loss: 3.142552]\n",
      "[Epoch 13/200] [Batch 606/938] [D loss: 0.391137] [G loss: 0.843037]\n",
      "[Epoch 14/200] [Batch 68/938] [D loss: 0.195604] [G loss: 2.462716]\n",
      "[Epoch 14/200] [Batch 468/938] [D loss: 0.503892] [G loss: 5.139613]\n",
      "[Epoch 14/200] [Batch 868/938] [D loss: 0.231432] [G loss: 2.426861]\n",
      "[Epoch 15/200] [Batch 330/938] [D loss: 0.149138] [G loss: 2.827064]\n",
      "[Epoch 15/200] [Batch 730/938] [D loss: 0.248125] [G loss: 3.257323]\n",
      "[Epoch 16/200] [Batch 192/938] [D loss: 0.158900] [G loss: 2.672740]\n",
      "[Epoch 16/200] [Batch 592/938] [D loss: 0.224106] [G loss: 1.662574]\n",
      "[Epoch 17/200] [Batch 54/938] [D loss: 0.272025] [G loss: 1.692824]\n",
      "[Epoch 17/200] [Batch 454/938] [D loss: 0.323600] [G loss: 4.228354]\n",
      "[Epoch 17/200] [Batch 854/938] [D loss: 0.137412] [G loss: 2.453204]\n",
      "[Epoch 18/200] [Batch 316/938] [D loss: 0.118076] [G loss: 2.016024]\n",
      "[Epoch 18/200] [Batch 716/938] [D loss: 0.188019] [G loss: 3.265843]\n",
      "[Epoch 19/200] [Batch 178/938] [D loss: 0.198742] [G loss: 3.248606]\n",
      "[Epoch 19/200] [Batch 578/938] [D loss: 0.421465] [G loss: 0.837832]\n",
      "[Epoch 20/200] [Batch 40/938] [D loss: 0.140635] [G loss: 2.983711]\n",
      "[Epoch 20/200] [Batch 440/938] [D loss: 0.315636] [G loss: 3.133166]\n",
      "[Epoch 20/200] [Batch 840/938] [D loss: 0.194346] [G loss: 1.982798]\n",
      "[Epoch 21/200] [Batch 302/938] [D loss: 0.247354] [G loss: 2.894556]\n",
      "[Epoch 21/200] [Batch 702/938] [D loss: 0.258441] [G loss: 1.807588]\n",
      "[Epoch 22/200] [Batch 164/938] [D loss: 0.192590] [G loss: 2.191725]\n",
      "[Epoch 22/200] [Batch 564/938] [D loss: 0.108374] [G loss: 2.534414]\n",
      "[Epoch 23/200] [Batch 26/938] [D loss: 0.173994] [G loss: 2.498311]\n",
      "[Epoch 23/200] [Batch 426/938] [D loss: 0.254488] [G loss: 4.124508]\n",
      "[Epoch 23/200] [Batch 826/938] [D loss: 0.210568] [G loss: 1.955335]\n",
      "[Epoch 24/200] [Batch 288/938] [D loss: 0.244544] [G loss: 1.352007]\n",
      "[Epoch 24/200] [Batch 688/938] [D loss: 0.146318] [G loss: 2.335608]\n",
      "[Epoch 25/200] [Batch 150/938] [D loss: 0.202493] [G loss: 2.396944]\n",
      "[Epoch 25/200] [Batch 550/938] [D loss: 0.506327] [G loss: 0.664664]\n",
      "[Epoch 26/200] [Batch 12/938] [D loss: 0.183662] [G loss: 1.851375]\n",
      "[Epoch 26/200] [Batch 412/938] [D loss: 0.152860] [G loss: 3.370724]\n",
      "[Epoch 26/200] [Batch 812/938] [D loss: 0.260075] [G loss: 1.612908]\n",
      "[Epoch 27/200] [Batch 274/938] [D loss: 0.201178] [G loss: 2.141769]\n",
      "[Epoch 27/200] [Batch 674/938] [D loss: 0.313325] [G loss: 1.197721]\n",
      "[Epoch 28/200] [Batch 136/938] [D loss: 0.159398] [G loss: 1.844835]\n",
      "[Epoch 28/200] [Batch 536/938] [D loss: 0.249376] [G loss: 1.732721]\n",
      "[Epoch 28/200] [Batch 936/938] [D loss: 0.195768] [G loss: 2.352266]\n",
      "[Epoch 29/200] [Batch 398/938] [D loss: 0.630573] [G loss: 4.867837]\n",
      "[Epoch 29/200] [Batch 798/938] [D loss: 0.229774] [G loss: 2.414746]\n",
      "[Epoch 30/200] [Batch 260/938] [D loss: 0.181103] [G loss: 2.462942]\n",
      "[Epoch 30/200] [Batch 660/938] [D loss: 0.189007] [G loss: 1.804534]\n",
      "[Epoch 31/200] [Batch 122/938] [D loss: 0.179045] [G loss: 2.968383]\n",
      "[Epoch 31/200] [Batch 522/938] [D loss: 0.162977] [G loss: 3.582447]\n",
      "[Epoch 31/200] [Batch 922/938] [D loss: 0.192647] [G loss: 3.066888]\n",
      "[Epoch 32/200] [Batch 384/938] [D loss: 0.166828] [G loss: 2.590515]\n",
      "[Epoch 32/200] [Batch 784/938] [D loss: 0.307263] [G loss: 1.393373]\n",
      "[Epoch 33/200] [Batch 246/938] [D loss: 0.212860] [G loss: 1.792936]\n",
      "[Epoch 33/200] [Batch 646/938] [D loss: 0.258148] [G loss: 3.366621]\n",
      "[Epoch 34/200] [Batch 108/938] [D loss: 0.267525] [G loss: 1.296049]\n",
      "[Epoch 34/200] [Batch 508/938] [D loss: 0.184159] [G loss: 2.375584]\n",
      "[Epoch 34/200] [Batch 908/938] [D loss: 0.232408] [G loss: 2.113122]\n",
      "[Epoch 35/200] [Batch 370/938] [D loss: 0.279044] [G loss: 1.435037]\n",
      "[Epoch 35/200] [Batch 770/938] [D loss: 0.211706] [G loss: 1.839050]\n",
      "[Epoch 36/200] [Batch 232/938] [D loss: 0.246680] [G loss: 1.860236]\n",
      "[Epoch 36/200] [Batch 632/938] [D loss: 0.551123] [G loss: 4.167905]\n",
      "[Epoch 37/200] [Batch 94/938] [D loss: 0.565834] [G loss: 3.701714]\n",
      "[Epoch 37/200] [Batch 494/938] [D loss: 0.357787] [G loss: 3.315567]\n",
      "[Epoch 37/200] [Batch 894/938] [D loss: 0.340172] [G loss: 1.336907]\n",
      "[Epoch 38/200] [Batch 356/938] [D loss: 0.271138] [G loss: 1.683006]\n",
      "[Epoch 38/200] [Batch 756/938] [D loss: 0.324250] [G loss: 2.169678]\n",
      "[Epoch 39/200] [Batch 218/938] [D loss: 0.369599] [G loss: 2.982042]\n",
      "[Epoch 39/200] [Batch 618/938] [D loss: 0.268162] [G loss: 1.792292]\n",
      "[Epoch 40/200] [Batch 80/938] [D loss: 0.236970] [G loss: 2.078148]\n",
      "[Epoch 40/200] [Batch 480/938] [D loss: 0.371194] [G loss: 3.742704]\n",
      "[Epoch 40/200] [Batch 880/938] [D loss: 0.251532] [G loss: 2.807871]\n",
      "[Epoch 41/200] [Batch 342/938] [D loss: 0.250363] [G loss: 1.667914]\n",
      "[Epoch 41/200] [Batch 742/938] [D loss: 0.282032] [G loss: 1.704704]\n",
      "[Epoch 42/200] [Batch 204/938] [D loss: 0.404512] [G loss: 3.279075]\n",
      "[Epoch 42/200] [Batch 604/938] [D loss: 0.268188] [G loss: 3.373920]\n",
      "[Epoch 43/200] [Batch 66/938] [D loss: 0.243615] [G loss: 2.304277]\n",
      "[Epoch 43/200] [Batch 466/938] [D loss: 0.251934] [G loss: 2.012841]\n",
      "[Epoch 43/200] [Batch 866/938] [D loss: 0.220061] [G loss: 2.288672]\n",
      "[Epoch 44/200] [Batch 328/938] [D loss: 0.321507] [G loss: 2.462090]\n",
      "[Epoch 44/200] [Batch 728/938] [D loss: 0.238245] [G loss: 2.042418]\n",
      "[Epoch 45/200] [Batch 190/938] [D loss: 0.226979] [G loss: 3.162184]\n",
      "[Epoch 45/200] [Batch 590/938] [D loss: 0.266507] [G loss: 1.645395]\n",
      "[Epoch 46/200] [Batch 52/938] [D loss: 0.241363] [G loss: 2.169646]\n",
      "[Epoch 46/200] [Batch 452/938] [D loss: 0.282347] [G loss: 3.509258]\n",
      "[Epoch 46/200] [Batch 852/938] [D loss: 0.195106] [G loss: 2.279716]\n",
      "[Epoch 47/200] [Batch 314/938] [D loss: 0.236424] [G loss: 2.234870]\n",
      "[Epoch 47/200] [Batch 714/938] [D loss: 0.294590] [G loss: 2.952621]\n",
      "[Epoch 48/200] [Batch 176/938] [D loss: 0.300604] [G loss: 2.873801]\n",
      "[Epoch 48/200] [Batch 576/938] [D loss: 0.308400] [G loss: 2.130862]\n",
      "[Epoch 49/200] [Batch 38/938] [D loss: 0.226323] [G loss: 2.491225]\n",
      "[Epoch 49/200] [Batch 438/938] [D loss: 0.270827] [G loss: 1.984359]\n",
      "[Epoch 49/200] [Batch 838/938] [D loss: 0.259383] [G loss: 2.232761]\n",
      "[Epoch 50/200] [Batch 300/938] [D loss: 0.219720] [G loss: 1.866442]\n",
      "[Epoch 50/200] [Batch 700/938] [D loss: 0.249700] [G loss: 1.635577]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 51/200] [Batch 162/938] [D loss: 0.215064] [G loss: 1.847807]\n",
      "[Epoch 51/200] [Batch 562/938] [D loss: 0.223668] [G loss: 2.226358]\n",
      "[Epoch 52/200] [Batch 24/938] [D loss: 0.322233] [G loss: 1.263673]\n",
      "[Epoch 52/200] [Batch 424/938] [D loss: 0.292803] [G loss: 1.770442]\n",
      "[Epoch 52/200] [Batch 824/938] [D loss: 0.232745] [G loss: 1.447896]\n",
      "[Epoch 53/200] [Batch 286/938] [D loss: 0.387485] [G loss: 1.678121]\n",
      "[Epoch 53/200] [Batch 686/938] [D loss: 0.341823] [G loss: 2.499058]\n",
      "[Epoch 54/200] [Batch 148/938] [D loss: 0.228127] [G loss: 2.517919]\n",
      "[Epoch 54/200] [Batch 548/938] [D loss: 0.222957] [G loss: 2.354793]\n",
      "[Epoch 55/200] [Batch 10/938] [D loss: 0.344666] [G loss: 3.054306]\n",
      "[Epoch 55/200] [Batch 410/938] [D loss: 0.430465] [G loss: 3.233263]\n",
      "[Epoch 55/200] [Batch 810/938] [D loss: 0.348050] [G loss: 1.567875]\n",
      "[Epoch 56/200] [Batch 272/938] [D loss: 0.336219] [G loss: 1.203178]\n",
      "[Epoch 56/200] [Batch 672/938] [D loss: 0.275431] [G loss: 1.809331]\n",
      "[Epoch 57/200] [Batch 134/938] [D loss: 0.343904] [G loss: 3.625780]\n",
      "[Epoch 57/200] [Batch 534/938] [D loss: 0.417664] [G loss: 1.173759]\n",
      "[Epoch 57/200] [Batch 934/938] [D loss: 0.304031] [G loss: 2.348446]\n",
      "[Epoch 58/200] [Batch 396/938] [D loss: 0.341172] [G loss: 4.087401]\n",
      "[Epoch 58/200] [Batch 796/938] [D loss: 0.264694] [G loss: 2.986190]\n",
      "[Epoch 59/200] [Batch 258/938] [D loss: 0.221061] [G loss: 2.952018]\n",
      "[Epoch 59/200] [Batch 658/938] [D loss: 0.231115] [G loss: 1.901936]\n",
      "[Epoch 60/200] [Batch 120/938] [D loss: 0.179960] [G loss: 1.827657]\n",
      "[Epoch 60/200] [Batch 520/938] [D loss: 0.277545] [G loss: 1.307802]\n",
      "[Epoch 60/200] [Batch 920/938] [D loss: 0.253262] [G loss: 2.612042]\n",
      "[Epoch 61/200] [Batch 382/938] [D loss: 0.348436] [G loss: 1.528091]\n",
      "[Epoch 61/200] [Batch 782/938] [D loss: 0.236940] [G loss: 2.978910]\n",
      "[Epoch 62/200] [Batch 244/938] [D loss: 0.277511] [G loss: 1.600747]\n",
      "[Epoch 62/200] [Batch 644/938] [D loss: 0.210904] [G loss: 2.720698]\n",
      "[Epoch 63/200] [Batch 106/938] [D loss: 0.370183] [G loss: 1.433910]\n",
      "[Epoch 63/200] [Batch 506/938] [D loss: 0.211584] [G loss: 2.354439]\n",
      "[Epoch 63/200] [Batch 906/938] [D loss: 0.286969] [G loss: 1.699038]\n",
      "[Epoch 64/200] [Batch 368/938] [D loss: 0.285442] [G loss: 3.239844]\n",
      "[Epoch 64/200] [Batch 768/938] [D loss: 0.172234] [G loss: 2.813736]\n",
      "[Epoch 65/200] [Batch 230/938] [D loss: 0.257548] [G loss: 3.298185]\n",
      "[Epoch 65/200] [Batch 630/938] [D loss: 0.233886] [G loss: 1.811704]\n",
      "[Epoch 66/200] [Batch 92/938] [D loss: 0.244219] [G loss: 2.349762]\n",
      "[Epoch 66/200] [Batch 492/938] [D loss: 0.283637] [G loss: 2.108682]\n",
      "[Epoch 66/200] [Batch 892/938] [D loss: 0.216753] [G loss: 2.447330]\n",
      "[Epoch 67/200] [Batch 354/938] [D loss: 0.192173] [G loss: 2.632014]\n",
      "[Epoch 67/200] [Batch 754/938] [D loss: 0.236630] [G loss: 1.670366]\n",
      "[Epoch 68/200] [Batch 216/938] [D loss: 0.328920] [G loss: 3.231740]\n",
      "[Epoch 68/200] [Batch 616/938] [D loss: 0.262924] [G loss: 2.311550]\n",
      "[Epoch 69/200] [Batch 78/938] [D loss: 0.415389] [G loss: 2.741087]\n",
      "[Epoch 69/200] [Batch 478/938] [D loss: 0.273901] [G loss: 1.905854]\n",
      "[Epoch 69/200] [Batch 878/938] [D loss: 0.303516] [G loss: 1.772574]\n",
      "[Epoch 70/200] [Batch 340/938] [D loss: 0.318317] [G loss: 1.473738]\n",
      "[Epoch 70/200] [Batch 740/938] [D loss: 0.366087] [G loss: 2.313454]\n",
      "[Epoch 71/200] [Batch 202/938] [D loss: 0.251655] [G loss: 2.362740]\n",
      "[Epoch 71/200] [Batch 602/938] [D loss: 0.444930] [G loss: 3.660198]\n",
      "[Epoch 72/200] [Batch 64/938] [D loss: 0.252556] [G loss: 2.775425]\n",
      "[Epoch 72/200] [Batch 464/938] [D loss: 0.205711] [G loss: 2.692197]\n",
      "[Epoch 72/200] [Batch 864/938] [D loss: 0.263406] [G loss: 2.214278]\n",
      "[Epoch 73/200] [Batch 326/938] [D loss: 0.253162] [G loss: 2.786969]\n",
      "[Epoch 73/200] [Batch 726/938] [D loss: 0.201604] [G loss: 2.029169]\n",
      "[Epoch 74/200] [Batch 188/938] [D loss: 0.317928] [G loss: 3.058400]\n",
      "[Epoch 74/200] [Batch 588/938] [D loss: 0.202541] [G loss: 1.531520]\n",
      "[Epoch 75/200] [Batch 50/938] [D loss: 0.175632] [G loss: 2.088866]\n",
      "[Epoch 75/200] [Batch 450/938] [D loss: 0.247977] [G loss: 2.159814]\n",
      "[Epoch 75/200] [Batch 850/938] [D loss: 0.156915] [G loss: 2.691061]\n",
      "[Epoch 76/200] [Batch 312/938] [D loss: 0.310939] [G loss: 1.482404]\n",
      "[Epoch 76/200] [Batch 712/938] [D loss: 0.311499] [G loss: 1.532650]\n",
      "[Epoch 77/200] [Batch 174/938] [D loss: 0.274511] [G loss: 2.088868]\n",
      "[Epoch 77/200] [Batch 574/938] [D loss: 0.252828] [G loss: 2.579101]\n",
      "[Epoch 78/200] [Batch 36/938] [D loss: 0.394229] [G loss: 1.556022]\n",
      "[Epoch 78/200] [Batch 436/938] [D loss: 0.272240] [G loss: 2.848719]\n",
      "[Epoch 78/200] [Batch 836/938] [D loss: 0.272243] [G loss: 2.743067]\n",
      "[Epoch 79/200] [Batch 298/938] [D loss: 0.228250] [G loss: 1.891309]\n",
      "[Epoch 79/200] [Batch 698/938] [D loss: 0.296487] [G loss: 1.616290]\n",
      "[Epoch 80/200] [Batch 160/938] [D loss: 0.276296] [G loss: 1.869587]\n",
      "[Epoch 80/200] [Batch 560/938] [D loss: 0.280652] [G loss: 1.883720]\n",
      "[Epoch 81/200] [Batch 22/938] [D loss: 0.269732] [G loss: 2.892593]\n",
      "[Epoch 81/200] [Batch 422/938] [D loss: 0.247565] [G loss: 2.369631]\n",
      "[Epoch 81/200] [Batch 822/938] [D loss: 0.229016] [G loss: 2.231502]\n",
      "[Epoch 82/200] [Batch 284/938] [D loss: 0.197714] [G loss: 2.299846]\n",
      "[Epoch 82/200] [Batch 684/938] [D loss: 0.341851] [G loss: 2.832872]\n",
      "[Epoch 83/200] [Batch 146/938] [D loss: 0.221590] [G loss: 2.314115]\n",
      "[Epoch 83/200] [Batch 546/938] [D loss: 0.271852] [G loss: 1.954626]\n",
      "[Epoch 84/200] [Batch 8/938] [D loss: 0.171337] [G loss: 2.208410]\n",
      "[Epoch 84/200] [Batch 408/938] [D loss: 0.333375] [G loss: 2.836127]\n",
      "[Epoch 84/200] [Batch 808/938] [D loss: 0.245881] [G loss: 2.769883]\n",
      "[Epoch 85/200] [Batch 270/938] [D loss: 0.249631] [G loss: 2.871981]\n",
      "[Epoch 85/200] [Batch 670/938] [D loss: 0.207296] [G loss: 2.535483]\n",
      "[Epoch 86/200] [Batch 132/938] [D loss: 0.236929] [G loss: 2.566507]\n",
      "[Epoch 86/200] [Batch 532/938] [D loss: 0.254433] [G loss: 1.954463]\n",
      "[Epoch 86/200] [Batch 932/938] [D loss: 0.343610] [G loss: 2.684356]\n",
      "[Epoch 87/200] [Batch 394/938] [D loss: 0.264225] [G loss: 2.710955]\n",
      "[Epoch 87/200] [Batch 794/938] [D loss: 0.262853] [G loss: 1.618376]\n",
      "[Epoch 88/200] [Batch 256/938] [D loss: 0.321181] [G loss: 2.209452]\n",
      "[Epoch 88/200] [Batch 656/938] [D loss: 0.241630] [G loss: 1.911955]\n",
      "[Epoch 89/200] [Batch 118/938] [D loss: 0.379444] [G loss: 2.773803]\n",
      "[Epoch 89/200] [Batch 518/938] [D loss: 0.304305] [G loss: 1.353903]\n",
      "[Epoch 89/200] [Batch 918/938] [D loss: 0.337465] [G loss: 1.632759]\n",
      "[Epoch 90/200] [Batch 380/938] [D loss: 0.271960] [G loss: 1.929846]\n",
      "[Epoch 90/200] [Batch 780/938] [D loss: 0.248659] [G loss: 2.694368]\n",
      "[Epoch 91/200] [Batch 242/938] [D loss: 0.254031] [G loss: 1.565874]\n",
      "[Epoch 91/200] [Batch 642/938] [D loss: 0.260308] [G loss: 2.011632]\n",
      "[Epoch 92/200] [Batch 104/938] [D loss: 0.250588] [G loss: 1.877755]\n",
      "[Epoch 92/200] [Batch 504/938] [D loss: 0.218766] [G loss: 1.952260]\n",
      "[Epoch 92/200] [Batch 904/938] [D loss: 0.179328] [G loss: 2.449126]\n",
      "[Epoch 93/200] [Batch 366/938] [D loss: 0.282816] [G loss: 2.818104]\n",
      "[Epoch 93/200] [Batch 766/938] [D loss: 0.145503] [G loss: 2.313968]\n",
      "[Epoch 94/200] [Batch 228/938] [D loss: 0.327414] [G loss: 2.594121]\n",
      "[Epoch 94/200] [Batch 628/938] [D loss: 0.241997] [G loss: 1.871028]\n",
      "[Epoch 95/200] [Batch 90/938] [D loss: 0.319585] [G loss: 2.113055]\n",
      "[Epoch 95/200] [Batch 490/938] [D loss: 0.294516] [G loss: 1.660567]\n",
      "[Epoch 95/200] [Batch 890/938] [D loss: 0.300981] [G loss: 1.977743]\n",
      "[Epoch 96/200] [Batch 352/938] [D loss: 0.320675] [G loss: 2.382551]\n",
      "[Epoch 96/200] [Batch 752/938] [D loss: 0.313646] [G loss: 1.712855]\n",
      "[Epoch 97/200] [Batch 214/938] [D loss: 0.249395] [G loss: 2.136054]\n",
      "[Epoch 97/200] [Batch 614/938] [D loss: 0.244877] [G loss: 1.882461]\n",
      "[Epoch 98/200] [Batch 76/938] [D loss: 0.295904] [G loss: 2.784821]\n",
      "[Epoch 98/200] [Batch 476/938] [D loss: 0.318520] [G loss: 2.898146]\n",
      "[Epoch 98/200] [Batch 876/938] [D loss: 0.374613] [G loss: 2.436974]\n",
      "[Epoch 99/200] [Batch 338/938] [D loss: 0.369637] [G loss: 3.657319]\n",
      "[Epoch 99/200] [Batch 738/938] [D loss: 0.233639] [G loss: 2.293838]\n",
      "[Epoch 100/200] [Batch 200/938] [D loss: 0.290747] [G loss: 2.372641]\n",
      "[Epoch 100/200] [Batch 600/938] [D loss: 0.264849] [G loss: 3.060960]\n",
      "[Epoch 101/200] [Batch 62/938] [D loss: 0.415862] [G loss: 3.121231]\n",
      "[Epoch 101/200] [Batch 462/938] [D loss: 0.297463] [G loss: 1.590577]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 101/200] [Batch 862/938] [D loss: 0.236271] [G loss: 1.882722]\n",
      "[Epoch 102/200] [Batch 324/938] [D loss: 0.340078] [G loss: 2.157157]\n",
      "[Epoch 102/200] [Batch 724/938] [D loss: 0.194775] [G loss: 2.210497]\n",
      "[Epoch 103/200] [Batch 186/938] [D loss: 0.319068] [G loss: 2.242239]\n",
      "[Epoch 103/200] [Batch 586/938] [D loss: 0.256296] [G loss: 1.803264]\n",
      "[Epoch 104/200] [Batch 48/938] [D loss: 0.296626] [G loss: 2.318252]\n",
      "[Epoch 104/200] [Batch 448/938] [D loss: 0.238718] [G loss: 2.452931]\n",
      "[Epoch 104/200] [Batch 848/938] [D loss: 0.261896] [G loss: 3.041712]\n",
      "[Epoch 105/200] [Batch 310/938] [D loss: 0.317626] [G loss: 2.463090]\n",
      "[Epoch 105/200] [Batch 710/938] [D loss: 0.231684] [G loss: 2.010285]\n",
      "[Epoch 106/200] [Batch 172/938] [D loss: 0.251759] [G loss: 2.187737]\n",
      "[Epoch 106/200] [Batch 572/938] [D loss: 0.274817] [G loss: 2.075555]\n",
      "[Epoch 107/200] [Batch 34/938] [D loss: 0.290542] [G loss: 2.586968]\n",
      "[Epoch 107/200] [Batch 434/938] [D loss: 0.248760] [G loss: 2.296454]\n",
      "[Epoch 107/200] [Batch 834/938] [D loss: 0.248900] [G loss: 2.778312]\n",
      "[Epoch 108/200] [Batch 296/938] [D loss: 0.331599] [G loss: 2.224344]\n",
      "[Epoch 108/200] [Batch 696/938] [D loss: 0.234268] [G loss: 1.828322]\n",
      "[Epoch 109/200] [Batch 158/938] [D loss: 0.326867] [G loss: 1.407701]\n",
      "[Epoch 109/200] [Batch 558/938] [D loss: 0.248908] [G loss: 1.650665]\n",
      "[Epoch 110/200] [Batch 20/938] [D loss: 0.289210] [G loss: 2.173807]\n",
      "[Epoch 110/200] [Batch 420/938] [D loss: 0.264630] [G loss: 1.647863]\n",
      "[Epoch 110/200] [Batch 820/938] [D loss: 0.350550] [G loss: 2.718961]\n",
      "[Epoch 111/200] [Batch 282/938] [D loss: 0.250329] [G loss: 2.205831]\n",
      "[Epoch 111/200] [Batch 682/938] [D loss: 0.265318] [G loss: 1.727596]\n",
      "[Epoch 112/200] [Batch 144/938] [D loss: 0.317132] [G loss: 1.886635]\n",
      "[Epoch 112/200] [Batch 544/938] [D loss: 0.428427] [G loss: 1.644745]\n",
      "[Epoch 113/200] [Batch 6/938] [D loss: 0.229047] [G loss: 1.994941]\n",
      "[Epoch 113/200] [Batch 406/938] [D loss: 0.297972] [G loss: 1.953372]\n",
      "[Epoch 113/200] [Batch 806/938] [D loss: 0.177687] [G loss: 2.897335]\n",
      "[Epoch 114/200] [Batch 268/938] [D loss: 0.356864] [G loss: 2.804523]\n",
      "[Epoch 114/200] [Batch 668/938] [D loss: 0.272825] [G loss: 2.515733]\n",
      "[Epoch 115/200] [Batch 130/938] [D loss: 0.235235] [G loss: 1.973411]\n",
      "[Epoch 115/200] [Batch 530/938] [D loss: 0.336908] [G loss: 2.758796]\n",
      "[Epoch 115/200] [Batch 930/938] [D loss: 0.251994] [G loss: 1.990309]\n",
      "[Epoch 116/200] [Batch 392/938] [D loss: 0.261761] [G loss: 1.892233]\n",
      "[Epoch 116/200] [Batch 792/938] [D loss: 0.276879] [G loss: 2.282065]\n",
      "[Epoch 117/200] [Batch 254/938] [D loss: 0.282137] [G loss: 2.450310]\n",
      "[Epoch 117/200] [Batch 654/938] [D loss: 0.228492] [G loss: 2.191131]\n",
      "[Epoch 118/200] [Batch 116/938] [D loss: 0.353456] [G loss: 1.801714]\n",
      "[Epoch 118/200] [Batch 516/938] [D loss: 0.280173] [G loss: 2.197010]\n",
      "[Epoch 118/200] [Batch 916/938] [D loss: 0.274693] [G loss: 2.554786]\n",
      "[Epoch 119/200] [Batch 378/938] [D loss: 0.280468] [G loss: 1.545153]\n",
      "[Epoch 119/200] [Batch 778/938] [D loss: 0.139064] [G loss: 2.222452]\n",
      "[Epoch 120/200] [Batch 240/938] [D loss: 0.271410] [G loss: 2.026860]\n",
      "[Epoch 120/200] [Batch 640/938] [D loss: 0.277620] [G loss: 2.583529]\n",
      "[Epoch 121/200] [Batch 102/938] [D loss: 0.299479] [G loss: 2.071416]\n",
      "[Epoch 121/200] [Batch 502/938] [D loss: 0.305761] [G loss: 1.632194]\n",
      "[Epoch 121/200] [Batch 902/938] [D loss: 0.278516] [G loss: 2.376008]\n",
      "[Epoch 122/200] [Batch 364/938] [D loss: 0.203131] [G loss: 2.283367]\n",
      "[Epoch 122/200] [Batch 764/938] [D loss: 0.464286] [G loss: 1.171194]\n",
      "[Epoch 123/200] [Batch 226/938] [D loss: 0.322228] [G loss: 2.185650]\n",
      "[Epoch 123/200] [Batch 626/938] [D loss: 0.234172] [G loss: 2.102259]\n",
      "[Epoch 124/200] [Batch 88/938] [D loss: 0.228765] [G loss: 1.921759]\n",
      "[Epoch 124/200] [Batch 488/938] [D loss: 0.265423] [G loss: 2.109913]\n",
      "[Epoch 124/200] [Batch 888/938] [D loss: 0.410236] [G loss: 1.944603]\n",
      "[Epoch 125/200] [Batch 350/938] [D loss: 0.340332] [G loss: 2.113289]\n",
      "[Epoch 125/200] [Batch 750/938] [D loss: 0.247597] [G loss: 1.836696]\n",
      "[Epoch 126/200] [Batch 212/938] [D loss: 0.284074] [G loss: 1.780120]\n",
      "[Epoch 126/200] [Batch 612/938] [D loss: 0.314624] [G loss: 1.864525]\n",
      "[Epoch 127/200] [Batch 74/938] [D loss: 0.277040] [G loss: 1.936925]\n",
      "[Epoch 127/200] [Batch 474/938] [D loss: 0.215180] [G loss: 2.196406]\n",
      "[Epoch 127/200] [Batch 874/938] [D loss: 0.266472] [G loss: 1.893846]\n",
      "[Epoch 128/200] [Batch 336/938] [D loss: 0.335272] [G loss: 1.738482]\n",
      "[Epoch 128/200] [Batch 736/938] [D loss: 0.384747] [G loss: 2.491873]\n",
      "[Epoch 129/200] [Batch 198/938] [D loss: 0.325687] [G loss: 1.595884]\n",
      "[Epoch 129/200] [Batch 598/938] [D loss: 0.229506] [G loss: 1.806544]\n",
      "[Epoch 130/200] [Batch 60/938] [D loss: 0.254017] [G loss: 2.472680]\n",
      "[Epoch 130/200] [Batch 460/938] [D loss: 0.259123] [G loss: 1.669172]\n",
      "[Epoch 130/200] [Batch 860/938] [D loss: 0.297199] [G loss: 1.817961]\n",
      "[Epoch 131/200] [Batch 322/938] [D loss: 0.361275] [G loss: 1.757273]\n",
      "[Epoch 131/200] [Batch 722/938] [D loss: 0.199434] [G loss: 2.224021]\n",
      "[Epoch 132/200] [Batch 184/938] [D loss: 0.294569] [G loss: 2.530365]\n",
      "[Epoch 132/200] [Batch 584/938] [D loss: 0.355989] [G loss: 3.152975]\n",
      "[Epoch 133/200] [Batch 46/938] [D loss: 0.262802] [G loss: 2.237722]\n",
      "[Epoch 133/200] [Batch 446/938] [D loss: 0.328227] [G loss: 2.142783]\n",
      "[Epoch 133/200] [Batch 846/938] [D loss: 0.415294] [G loss: 1.288860]\n",
      "[Epoch 134/200] [Batch 308/938] [D loss: 0.311722] [G loss: 2.291693]\n",
      "[Epoch 134/200] [Batch 708/938] [D loss: 0.254160] [G loss: 2.111582]\n",
      "[Epoch 135/200] [Batch 170/938] [D loss: 0.385862] [G loss: 2.044062]\n",
      "[Epoch 135/200] [Batch 570/938] [D loss: 0.201365] [G loss: 2.530943]\n",
      "[Epoch 136/200] [Batch 32/938] [D loss: 0.258054] [G loss: 1.678996]\n",
      "[Epoch 136/200] [Batch 432/938] [D loss: 0.285365] [G loss: 1.685198]\n",
      "[Epoch 136/200] [Batch 832/938] [D loss: 0.330726] [G loss: 1.393019]\n",
      "[Epoch 137/200] [Batch 294/938] [D loss: 0.290017] [G loss: 1.745108]\n",
      "[Epoch 137/200] [Batch 694/938] [D loss: 0.233343] [G loss: 1.892813]\n",
      "[Epoch 138/200] [Batch 156/938] [D loss: 0.235988] [G loss: 1.889479]\n",
      "[Epoch 138/200] [Batch 556/938] [D loss: 0.294700] [G loss: 2.035747]\n",
      "[Epoch 139/200] [Batch 18/938] [D loss: 0.273678] [G loss: 1.929000]\n",
      "[Epoch 139/200] [Batch 418/938] [D loss: 0.280569] [G loss: 2.358175]\n",
      "[Epoch 139/200] [Batch 818/938] [D loss: 0.304646] [G loss: 2.049594]\n",
      "[Epoch 140/200] [Batch 280/938] [D loss: 0.339076] [G loss: 2.009604]\n",
      "[Epoch 140/200] [Batch 680/938] [D loss: 0.303394] [G loss: 2.426268]\n",
      "[Epoch 141/200] [Batch 142/938] [D loss: 0.353436] [G loss: 1.547243]\n",
      "[Epoch 141/200] [Batch 542/938] [D loss: 0.340952] [G loss: 1.644258]\n",
      "[Epoch 142/200] [Batch 4/938] [D loss: 0.362764] [G loss: 1.599358]\n",
      "[Epoch 142/200] [Batch 404/938] [D loss: 0.253248] [G loss: 2.630800]\n",
      "[Epoch 142/200] [Batch 804/938] [D loss: 0.341850] [G loss: 1.663471]\n",
      "[Epoch 143/200] [Batch 266/938] [D loss: 0.341020] [G loss: 2.151403]\n",
      "[Epoch 143/200] [Batch 666/938] [D loss: 0.378327] [G loss: 1.784182]\n",
      "[Epoch 144/200] [Batch 128/938] [D loss: 0.285016] [G loss: 1.554659]\n",
      "[Epoch 144/200] [Batch 528/938] [D loss: 0.374966] [G loss: 2.533587]\n",
      "[Epoch 144/200] [Batch 928/938] [D loss: 0.366091] [G loss: 1.502930]\n",
      "[Epoch 145/200] [Batch 390/938] [D loss: 0.447649] [G loss: 2.157265]\n",
      "[Epoch 145/200] [Batch 790/938] [D loss: 0.295626] [G loss: 1.695589]\n",
      "[Epoch 146/200] [Batch 252/938] [D loss: 0.304524] [G loss: 2.153420]\n",
      "[Epoch 146/200] [Batch 652/938] [D loss: 0.297649] [G loss: 2.013176]\n",
      "[Epoch 147/200] [Batch 114/938] [D loss: 0.319054] [G loss: 2.387749]\n",
      "[Epoch 147/200] [Batch 514/938] [D loss: 0.254317] [G loss: 1.668248]\n",
      "[Epoch 147/200] [Batch 914/938] [D loss: 0.250091] [G loss: 1.983111]\n",
      "[Epoch 148/200] [Batch 376/938] [D loss: 0.325280] [G loss: 2.081412]\n",
      "[Epoch 148/200] [Batch 776/938] [D loss: 0.353018] [G loss: 1.646631]\n",
      "[Epoch 149/200] [Batch 238/938] [D loss: 0.260430] [G loss: 2.092790]\n",
      "[Epoch 149/200] [Batch 638/938] [D loss: 0.301880] [G loss: 2.297064]\n",
      "[Epoch 150/200] [Batch 100/938] [D loss: 0.312538] [G loss: 1.894493]\n",
      "[Epoch 150/200] [Batch 500/938] [D loss: 0.405300] [G loss: 1.639236]\n",
      "[Epoch 150/200] [Batch 900/938] [D loss: 0.358356] [G loss: 1.750442]\n",
      "[Epoch 151/200] [Batch 362/938] [D loss: 0.355148] [G loss: 1.939872]\n",
      "[Epoch 151/200] [Batch 762/938] [D loss: 0.349833] [G loss: 1.906582]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 152/200] [Batch 224/938] [D loss: 0.321077] [G loss: 2.086518]\n",
      "[Epoch 152/200] [Batch 624/938] [D loss: 0.333415] [G loss: 2.048362]\n",
      "[Epoch 153/200] [Batch 86/938] [D loss: 0.328353] [G loss: 2.358866]\n",
      "[Epoch 153/200] [Batch 486/938] [D loss: 0.429424] [G loss: 2.313407]\n",
      "[Epoch 153/200] [Batch 886/938] [D loss: 0.352999] [G loss: 1.606602]\n",
      "[Epoch 154/200] [Batch 348/938] [D loss: 0.333544] [G loss: 1.747373]\n",
      "[Epoch 154/200] [Batch 748/938] [D loss: 0.281827] [G loss: 2.066483]\n",
      "[Epoch 155/200] [Batch 210/938] [D loss: 0.313337] [G loss: 1.611214]\n",
      "[Epoch 155/200] [Batch 610/938] [D loss: 0.268371] [G loss: 1.873268]\n",
      "[Epoch 156/200] [Batch 72/938] [D loss: 0.371652] [G loss: 2.015953]\n",
      "[Epoch 156/200] [Batch 472/938] [D loss: 0.299577] [G loss: 1.681162]\n",
      "[Epoch 156/200] [Batch 872/938] [D loss: 0.365319] [G loss: 1.458398]\n",
      "[Epoch 157/200] [Batch 334/938] [D loss: 0.404813] [G loss: 2.423235]\n",
      "[Epoch 157/200] [Batch 734/938] [D loss: 0.317120] [G loss: 1.803173]\n",
      "[Epoch 158/200] [Batch 196/938] [D loss: 0.289089] [G loss: 1.516844]\n",
      "[Epoch 158/200] [Batch 596/938] [D loss: 0.329335] [G loss: 1.895784]\n",
      "[Epoch 159/200] [Batch 58/938] [D loss: 0.328503] [G loss: 1.528256]\n",
      "[Epoch 159/200] [Batch 458/938] [D loss: 0.333444] [G loss: 1.607927]\n",
      "[Epoch 159/200] [Batch 858/938] [D loss: 0.281511] [G loss: 1.590195]\n",
      "[Epoch 160/200] [Batch 320/938] [D loss: 0.334868] [G loss: 1.712050]\n",
      "[Epoch 160/200] [Batch 720/938] [D loss: 0.321138] [G loss: 1.746279]\n",
      "[Epoch 161/200] [Batch 182/938] [D loss: 0.332546] [G loss: 1.646912]\n",
      "[Epoch 161/200] [Batch 582/938] [D loss: 0.397111] [G loss: 1.768983]\n",
      "[Epoch 162/200] [Batch 44/938] [D loss: 0.333100] [G loss: 1.796293]\n",
      "[Epoch 162/200] [Batch 444/938] [D loss: 0.413776] [G loss: 2.185445]\n",
      "[Epoch 162/200] [Batch 844/938] [D loss: 0.345049] [G loss: 1.858950]\n",
      "[Epoch 163/200] [Batch 306/938] [D loss: 0.376998] [G loss: 1.233775]\n",
      "[Epoch 163/200] [Batch 706/938] [D loss: 0.417552] [G loss: 1.313363]\n",
      "[Epoch 164/200] [Batch 168/938] [D loss: 0.332130] [G loss: 1.489743]\n",
      "[Epoch 164/200] [Batch 568/938] [D loss: 0.347610] [G loss: 1.786295]\n",
      "[Epoch 165/200] [Batch 30/938] [D loss: 0.312393] [G loss: 1.722094]\n",
      "[Epoch 165/200] [Batch 430/938] [D loss: 0.428481] [G loss: 1.582260]\n",
      "[Epoch 165/200] [Batch 830/938] [D loss: 0.272525] [G loss: 1.789381]\n",
      "[Epoch 166/200] [Batch 292/938] [D loss: 0.364241] [G loss: 1.767415]\n",
      "[Epoch 166/200] [Batch 692/938] [D loss: 0.228334] [G loss: 1.627731]\n",
      "[Epoch 167/200] [Batch 154/938] [D loss: 0.289028] [G loss: 1.788596]\n",
      "[Epoch 167/200] [Batch 554/938] [D loss: 0.190722] [G loss: 1.787721]\n",
      "[Epoch 168/200] [Batch 16/938] [D loss: 0.365519] [G loss: 1.591146]\n",
      "[Epoch 168/200] [Batch 416/938] [D loss: 0.297874] [G loss: 1.506531]\n",
      "[Epoch 168/200] [Batch 816/938] [D loss: 0.342957] [G loss: 1.362555]\n",
      "[Epoch 169/200] [Batch 278/938] [D loss: 0.366128] [G loss: 1.909492]\n",
      "[Epoch 169/200] [Batch 678/938] [D loss: 0.354899] [G loss: 1.535098]\n",
      "[Epoch 170/200] [Batch 140/938] [D loss: 0.345728] [G loss: 1.640909]\n",
      "[Epoch 170/200] [Batch 540/938] [D loss: 0.381803] [G loss: 2.059202]\n",
      "[Epoch 171/200] [Batch 2/938] [D loss: 0.352572] [G loss: 1.858501]\n",
      "[Epoch 171/200] [Batch 402/938] [D loss: 0.336770] [G loss: 1.768647]\n",
      "[Epoch 171/200] [Batch 802/938] [D loss: 0.335092] [G loss: 1.861132]\n",
      "[Epoch 172/200] [Batch 264/938] [D loss: 0.300186] [G loss: 1.855361]\n",
      "[Epoch 172/200] [Batch 664/938] [D loss: 0.362635] [G loss: 1.708753]\n",
      "[Epoch 173/200] [Batch 126/938] [D loss: 0.406188] [G loss: 2.403086]\n",
      "[Epoch 173/200] [Batch 526/938] [D loss: 0.403904] [G loss: 2.126820]\n",
      "[Epoch 173/200] [Batch 926/938] [D loss: 0.362061] [G loss: 1.475976]\n",
      "[Epoch 174/200] [Batch 388/938] [D loss: 0.399185] [G loss: 1.554153]\n",
      "[Epoch 174/200] [Batch 788/938] [D loss: 0.397487] [G loss: 1.629273]\n",
      "[Epoch 175/200] [Batch 250/938] [D loss: 0.396824] [G loss: 1.619052]\n",
      "[Epoch 175/200] [Batch 650/938] [D loss: 0.335294] [G loss: 1.778744]\n",
      "[Epoch 176/200] [Batch 112/938] [D loss: 0.388072] [G loss: 1.915323]\n",
      "[Epoch 176/200] [Batch 512/938] [D loss: 0.472129] [G loss: 1.899466]\n",
      "[Epoch 176/200] [Batch 912/938] [D loss: 0.349848] [G loss: 1.635406]\n",
      "[Epoch 177/200] [Batch 374/938] [D loss: 0.338937] [G loss: 1.783648]\n",
      "[Epoch 177/200] [Batch 774/938] [D loss: 0.382741] [G loss: 1.894405]\n",
      "[Epoch 178/200] [Batch 236/938] [D loss: 0.296418] [G loss: 1.967339]\n",
      "[Epoch 178/200] [Batch 636/938] [D loss: 0.342230] [G loss: 1.687527]\n",
      "[Epoch 179/200] [Batch 98/938] [D loss: 0.356631] [G loss: 1.734126]\n",
      "[Epoch 179/200] [Batch 498/938] [D loss: 0.358881] [G loss: 1.424760]\n",
      "[Epoch 179/200] [Batch 898/938] [D loss: 0.364988] [G loss: 1.621414]\n",
      "[Epoch 180/200] [Batch 360/938] [D loss: 0.398439] [G loss: 1.713458]\n",
      "[Epoch 180/200] [Batch 760/938] [D loss: 0.343862] [G loss: 1.554220]\n",
      "[Epoch 181/200] [Batch 222/938] [D loss: 0.370850] [G loss: 2.133495]\n",
      "[Epoch 181/200] [Batch 622/938] [D loss: 0.326400] [G loss: 1.551926]\n",
      "[Epoch 182/200] [Batch 84/938] [D loss: 0.324598] [G loss: 1.456005]\n",
      "[Epoch 182/200] [Batch 484/938] [D loss: 0.364834] [G loss: 1.823941]\n",
      "[Epoch 182/200] [Batch 884/938] [D loss: 0.298256] [G loss: 1.841156]\n",
      "[Epoch 183/200] [Batch 346/938] [D loss: 0.303653] [G loss: 2.061678]\n",
      "[Epoch 183/200] [Batch 746/938] [D loss: 0.360612] [G loss: 1.998327]\n",
      "[Epoch 184/200] [Batch 208/938] [D loss: 0.329418] [G loss: 1.646034]\n",
      "[Epoch 184/200] [Batch 608/938] [D loss: 0.451265] [G loss: 1.655957]\n",
      "[Epoch 185/200] [Batch 70/938] [D loss: 0.312615] [G loss: 2.222114]\n",
      "[Epoch 185/200] [Batch 470/938] [D loss: 0.295384] [G loss: 1.717246]\n",
      "[Epoch 185/200] [Batch 870/938] [D loss: 0.342939] [G loss: 1.410932]\n",
      "[Epoch 186/200] [Batch 332/938] [D loss: 0.315627] [G loss: 1.901297]\n",
      "[Epoch 186/200] [Batch 732/938] [D loss: 0.274656] [G loss: 1.861116]\n",
      "[Epoch 187/200] [Batch 194/938] [D loss: 0.453950] [G loss: 1.676203]\n",
      "[Epoch 187/200] [Batch 594/938] [D loss: 0.393429] [G loss: 1.443897]\n",
      "[Epoch 188/200] [Batch 56/938] [D loss: 0.348901] [G loss: 1.648833]\n",
      "[Epoch 188/200] [Batch 456/938] [D loss: 0.260168] [G loss: 1.659569]\n",
      "[Epoch 188/200] [Batch 856/938] [D loss: 0.322012] [G loss: 1.304418]\n",
      "[Epoch 189/200] [Batch 318/938] [D loss: 0.332438] [G loss: 1.528204]\n",
      "[Epoch 189/200] [Batch 718/938] [D loss: 0.395120] [G loss: 1.818149]\n"
     ]
    }
   ],
   "source": [
    "g_loss_list = []\n",
    "d_loss_list = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False)\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(Tensor))\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "        optimizer_G.zero_grad()\n",
    "        # Sample noise as generator input\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z)\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        #----------------------\n",
    "        # Generator Grad Overlap (In Progress)\n",
    "        # ---------------------\n",
    "        #x = Variable(torch.Tensor([1, 1]), requires_grad=True)\n",
    "        #v = x.clone().detach()\n",
    "        #f = 3*x[0]**2 + 4*x[0]*x[1] + x[1]**2\n",
    "        #grad_f, = torch.autograd.grad(f, x, create_graph=True)\n",
    "        #z = grad_f @ v\n",
    "        #z.backward()\n",
    "        #print(x.grad)\n",
    "        \n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        optimizer_D.zero_grad()\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # ---------------------\n",
    "        # Monitor Progress\n",
    "        # ---------------------\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        if batches_done % sample_interval == 0:# and batches_done != 0:\n",
    "            print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\" % \\\n",
    "                   (epoch, n_epochs, i, len(dataloader), d_loss.item(), g_loss.item()))\n",
    "            \n",
    "            batches_done_str = str(batches_done).zfill(8)\n",
    "            \n",
    "            ## save the losses\n",
    "            g_loss_list.append(g_loss.item())\n",
    "            d_loss_list.append(d_loss.item())\n",
    "            \n",
    "            # plot a grid of samples\n",
    "            x = gen_imgs.data[:,0,:,:].cpu()\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            plt.suptitle(\"Sampled Images, Batch_Num: \" + str(batches_done), fontsize=16)\n",
    "            for j in range(64):\n",
    "                plt.subplot(8, 8, j + 1)\n",
    "                plt.imshow(x[j].reshape([28,28]), cmap='gray')\n",
    "                plt.xticks(())\n",
    "                plt.yticks(())\n",
    "            plt.savefig('images/sample_images_' + batches_done_str + '.png')\n",
    "            plt.close()\n",
    "            #plt.show()\n",
    "\n",
    "            Z = Variable(Tensor(np.random.normal(0, 1, (1000, latent_dim))))\n",
    "            gen_imgs = generator(Z)\n",
    "            X = gen_imgs.data[:,0,:,:].cpu().numpy()\n",
    "\n",
    "            [K_distances_l2, K_angles_l2, K_distances_H, K_angles_H] = triangle_distributions(X, 10000)\n",
    "            \n",
    "            ## plot the 2d triangle distance density plot (l2 norm)\n",
    "            fig,ax = plt.subplots(1,1)\n",
    "            ax1 = sns.kdeplot(K_distances_l2[:,0], K_distances_l2[:,1])\n",
    "            ax1.set_xlim([0, 1.0])\n",
    "            ax1.set_ylim([0, 1.0])\n",
    "            ax1.set_xlabel(r\"$d_{mid}-d_{min}$\", fontsize=16)\n",
    "            ax1.set_ylabel(r\"$d_{max}-d_{mid}$\", fontsize=16)\n",
    "            ax1.set_title(\"Distance Density Plot (l2 distance), Batch_Num: \" + str(batches_done), fontsize=16)\n",
    "            plt.savefig('images/distance_plot_l2_' + batches_done_str + '.png')\n",
    "            plt.close()\n",
    "            #plt.show()\n",
    "            \n",
    "            ## plot the 2d triangle angle density plot (l2 norm)\n",
    "            fig,ax = plt.subplots(1,1)\n",
    "            ax2 = sns.kdeplot(K_angles_l2[:,0], K_angles_l2[:,1])\n",
    "            ax2.set_xlim([0, 1.0])\n",
    "            ax2.set_ylim([0, 1.0])\n",
    "            ax2.set_xlabel(r\"$\\theta_{min}/\\theta_{mid}$\", fontsize=16)\n",
    "            ax2.set_ylabel(r\"$\\theta_{min}/\\theta_{max}$\", fontsize=16)\n",
    "            ax2.set_title(\"Angle Density Plot (l2 distance), Batch_Num: \" + str(batches_done), fontsize=16)\n",
    "            plt.savefig('images/triangle_plot_l2_' + batches_done_str + '.png')\n",
    "            plt.close()            \n",
    "            #plt.show()\n",
    "    \n",
    "            ## plot the 2d triangle distance density plot (H norm)\n",
    "            fig,ax = plt.subplots(1,1)\n",
    "            ax1 = sns.kdeplot(K_distances_H[:,0], K_distances_H[:,1])\n",
    "            ax1.set_xlim([0, 1.0])\n",
    "            ax1.set_ylim([0, 1.0])\n",
    "            ax1.set_xlabel(r\"$d_{mid}-d_{min}$\", fontsize=16)\n",
    "            ax1.set_ylabel(r\"$d_{max}-d_{mid}$\", fontsize=16)\n",
    "            ax1.set_title(\"Distance Density Plot (H distance), Batch_Num: \" + str(batches_done), fontsize=16)\n",
    "            plt.savefig('images/distance_plot_H_' + batches_done_str + '.png')\n",
    "            plt.close()\n",
    "            #plt.show()\n",
    "            \n",
    "            ## plot the 2d triangle angle density plot (H norm)\n",
    "            fig,ax = plt.subplots(1,1)\n",
    "            ax2 = sns.kdeplot(K_angles_H[:,0], K_angles_H[:,1])\n",
    "            ax2.set_xlim([0, 1.0])\n",
    "            ax2.set_ylim([0, 1.0])\n",
    "            ax2.set_xlabel(r\"$\\theta_{min}/\\theta_{mid}$\", fontsize=16)\n",
    "            ax2.set_ylabel(r\"$\\theta_{min}/\\theta_{max}$\", fontsize=16)\n",
    "            ax2.set_title(\"Angle Density Plot (H distance), Batch_Num: \" + str(batches_done), fontsize=16)\n",
    "            plt.savefig('images/triangle_plot_H_' + batches_done_str + '.png')\n",
    "            plt.close()            \n",
    "            #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Snippets - Work in Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the norm of the gradient\n",
    "#g_grad_norm = Variable(Tensor(1).fill_(0.0), requires_grad=True)\n",
    "#for p in generator.parameters():\n",
    "#    g_grad_norm += p.grad.data.norm(2).item()**2\n",
    "#g_grad_norm = g_grad_norm **(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## look at 1d measures of UM\n",
    "## look at raw MNIST data\n",
    "## double check code\n",
    "## look into top-subspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to take the \"Hessian vector product\", i.e. $ \\sum_j H_{ij} v_j $ for $H_{ij} = \\partial_i \\partial_j f$ and $v_j$ an arbitrary vector. \n",
    "\n",
    "This came from this PyTorch help forum post: https://discuss.pytorch.org/t/calculating-hessian-vector-product/11240/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## first, let v be an arbitrary vector:\n",
    "v = Variable(torch.Tensor([1, 1]))\n",
    "x = Variable(torch.Tensor([1, 1]), requires_grad=True)\n",
    "f = 3*x[0]**2 + 4*x[0]*x[1] + x[1]**2\n",
    "grad_f, = torch.autograd.grad(f, x, create_graph=True)\n",
    "z = grad_f @ v\n",
    "z.backward()\n",
    "print(x.grad)\n",
    "\n",
    "## now, let v be x - note that now the answer changes because the gradient also hits v\n",
    "x = Variable(torch.Tensor([1, 1]), requires_grad=True)\n",
    "v = x\n",
    "f = 3*x[0]**2 + 4*x[0]*x[1] + x[1]**2\n",
    "grad_f, = torch.autograd.grad(f, x, create_graph=True)\n",
    "z = grad_f @ v\n",
    "z.backward()\n",
    "print(x.grad)\n",
    "\n",
    "## lastly, clone + detach v so that the derivative does not hit it, even though it is related to x\n",
    "x = Variable(torch.Tensor([1, 1]), requires_grad=True)\n",
    "v = x.clone().detach()\n",
    "f = 3*x[0]**2 + 4*x[0]*x[1] + x[1]**2\n",
    "grad_f, = torch.autograd.grad(f, x, create_graph=True)\n",
    "z = grad_f @ v\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lastly, clone + detach v so that the derivative does not hit it, even though it is related to x\n",
    "x = Variable(torch.Tensor([1, 1]), requires_grad=True)\n",
    "f = 3*x[0]**2 + 4*x[0]*x[1] + x[1]**2\n",
    "grad_f, = torch.autograd.grad(f, x, create_graph=True)\n",
    "v = grad_f.clone().detach()\n",
    "z = grad_f @ v\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n",
    "gen_imgs = generator(z)\n",
    "g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "\n",
    "theta_g_tmp = []\n",
    "for param in generator.parameters():\n",
    "    theta_g_tmp.append(param.view(-1))\n",
    "theta_g = torch.cat(theta_g_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grad_g  = torch.autograd.grad(g_loss, generator.parameters(), create_graph=True)\n",
    "grad_g  = torch.autograd.grad(g_loss, theta_g, create_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "theta_g_tmp = []\n",
    "for param in generator.parameters():\n",
    "    theta_g_tmp.append(param.view(-1))\n",
    "theta_g = torch.cat(theta_g_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = grad_g @ theta_g\n",
    "z.backward()\n",
    "print(theta_g.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(10, 20)\n",
    "input = torch.randn(1, 10)\n",
    "out = linear(input).sum()\n",
    "grads = torch.autograd.grad([out], linear.parameters(), create_graph=True)\n",
    "flatten = torch.cat([g.reshape(-1) for g in grads if g is not None])\n",
    "x = torch.randn_like(flatten)\n",
    "print(flatten.shape)\n",
    "flatten2 = Variable(flatten.data, requires_grad=True)\n",
    "hvps = torch.autograd.grad([flatten2 @ x], linear.parameters(), allow_unused=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn_like(flatten)\n",
    "print(flatten.shape) ## torch.Size([1792])\n",
    "x2 = Variable(x.data, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvps = torch.autograd.grad([flatten @ x2], conv.parameters(), allow_unused=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hvps[1]) ## None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten2 = torch.cat([g.reshape(-1) for g in hvps if g is not None])\n",
    "print(flatten2.shape) ## torch.Size([1728])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## a simple neural network\n",
    "linear = nn.Linear(10, 20)\n",
    "x = torch.randn(1, 10)\n",
    "y = linear(x).sum()\n",
    "\n",
    "## compute the gradient and make a copy that is detached from the graph\n",
    "grad = torch.autograd.grad(y, linear.parameters(), create_graph=True)\n",
    "v = grad.clone().detach()\n",
    "\n",
    "## compute the Hessian vector product\n",
    "z = grad @ v\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lastly, clone + detach v so that the derivative does not hit it, even though it is related to x\n",
    "x = Variable(torch.Tensor([1, 1]), requires_grad=True)\n",
    "v = x.clone().detach()\n",
    "f = 3*x[0]**2 + 4*x[0]*x[1] + x[1]**2\n",
    "grad_f, = torch.autograd.grad(f, x, create_graph=True)\n",
    "z = grad_f @ v\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad.view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvps = torch.autograd.grad([flatten2 @ x], linear.parameters(), allow_unused=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = torch.cat([g.reshape(-1) for g in grads if g is not None])\n",
    "x = torch.randn_like(flatten)\n",
    "print(flatten.shape)\n",
    "flatten2 = Variable(flatten.data, requires_grad=True)\n",
    "hvps = torch.autograd.grad([flatten2 @ x], linear.parameters(), allow_unused=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lastly, clone + detach v so that the derivative does not hit it, even though it is related to x\n",
    "x = Variable(torch.Tensor([1, 1]), requires_grad=True)\n",
    "v = x.clone().detach()\n",
    "f = 3*x[0]**2 + 4*x[0]*x[1] + x[1]**2\n",
    "grad_f, = torch.autograd.grad(f, x, create_graph=True)\n",
    "z = grad_f @ v\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_f @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PyTorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
