{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adapted from the GAN implementation in the PyTorch-GAN model zoo:\n",
    "## https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_style(\"white\")\n",
    "%matplotlib inline\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('images', exist_ok=True)\n",
    "\n",
    "n_epochs = 200 #'number of epochs of training'\n",
    "batch_size = 64 #'size of the batches'\n",
    "lr = 0.0002 #'adam: learning rate'\n",
    "b1 = 0.5 #'adam: decay of first order momentum of gradient'\n",
    "b2 = 0.999 #'adam: decay of first order momentum of gradient'\n",
    "n_cpu = 4 #'number of cpu threads to use during batch generation'\n",
    "latent_dim = 100 #'dimensionality of the latent space'\n",
    "img_size = 28 #'size of each image dimension'\n",
    "channels = 1 #'number of image channels'\n",
    "sample_interval=400 #'interval betwen image samples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (channels, img_size, img_size)\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(latent_dim, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), *img_shape)\n",
    "        return img\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        validity = self.model(img_flat)\n",
    "\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()\n",
    "\n",
    "# Configure data loader\n",
    "os.makedirs('../../data/mnist', exist_ok=True)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../../data/mnist', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(x,y):\n",
    "    \"\"\"\n",
    "    given 2 images, x, y, return the normalized l2 distance between them,\n",
    "    normalized so that the max distance is 1\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum((x - y)**2))/56\n",
    "\n",
    "\n",
    "def triangle_distances(x,y,z):\n",
    "    \"\"\"\n",
    "    return the sorted distances of the triangle formed by the points (x,y,z)\n",
    "    \"\"\"\n",
    "    return sorted([distance(x,y), distance(x,z), distance(y,z)])\n",
    "\n",
    "\n",
    "def triplet_sample(X):\n",
    "    \"\"\"\n",
    "    given an array of images X, return a triplet\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        [i,j,k] = list(np.random.randint(0, X.shape[0], 3))\n",
    "        if len({i,j,k}) == 3:\n",
    "            return [X[i], X[j], X[k]]\n",
    "\n",
    "        \n",
    "def angles(x, y, z):\n",
    "    \"\"\"\n",
    "    Given a triangle-defining triplet, return the sorted angles\n",
    "    \"\"\"\n",
    "    [dxy, dxz, dyz] = triangle_distances(x,y,z)\n",
    "    theta_xy = np.arccos((-dxy**2 + dxz**2 + dyz**2)/(2*dxz*dyz))\n",
    "    theta_xz = np.arccos((dxy**2 - dxz**2 + dyz**2)/(2*dxy*dyz))\n",
    "    theta_yz = np.arccos((dxy**2 + dxz**2 - dyz**2)/(2*dxy*dxz))\n",
    "    return sorted([theta_xy, theta_xz, theta_yz])\n",
    "\n",
    "\n",
    "def triangle_distributions(X, Num):\n",
    "    \"\"\"\n",
    "    Given an array of samples X, generate two arrays of shape (Num, 2).\n",
    "    \n",
    "    Array 1: the 2 dimensions are [dmid-dmin, dmax-dmid]\n",
    "    Array 2: the 2 dimensions are [theta_min/theta_max, theta_min/theta_mid]\n",
    "    \"\"\"\n",
    "\n",
    "    K_distances = np.zeros((Num,2))\n",
    "    K_angles = np.zeros((Num,2))\n",
    "    \n",
    "    for i in range(Num):\n",
    "        [x, y, z] = triplet_sample(X)\n",
    "        [d_min, d_mid, d_max] = triangle_distances(x,y,z)\n",
    "        [theta_min, theta_mid, theta_max] = angles(x,y,z)\n",
    "        \n",
    "        K_distances[i,0] = d_mid - d_min\n",
    "        K_distances[i,1] = d_max - d_mid\n",
    "        \n",
    "        K_angles[i,0] = theta_min/theta_mid\n",
    "        K_angles[i,1] = theta_min/theta_max\n",
    "\n",
    "        \n",
    "    return [K_distances, K_angles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 0/938] [D loss: 0.653889] [G loss: 0.718994]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 400/938] [D loss: 0.483200] [G loss: 0.578756]\n",
      "[Epoch 0/200] [Batch 800/938] [D loss: 0.342723] [G loss: 0.822985]\n",
      "[Epoch 1/200] [Batch 262/938] [D loss: 0.588816] [G loss: 2.485917]\n",
      "[Epoch 1/200] [Batch 662/938] [D loss: 0.484637] [G loss: 0.761122]\n",
      "[Epoch 2/200] [Batch 124/938] [D loss: 0.290944] [G loss: 1.087301]\n",
      "[Epoch 2/200] [Batch 524/938] [D loss: 0.607129] [G loss: 0.460838]\n",
      "[Epoch 2/200] [Batch 924/938] [D loss: 0.162961] [G loss: 1.576303]\n",
      "[Epoch 3/200] [Batch 386/938] [D loss: 0.264576] [G loss: 3.198326]\n",
      "[Epoch 3/200] [Batch 786/938] [D loss: 0.198144] [G loss: 2.381291]\n",
      "[Epoch 4/200] [Batch 248/938] [D loss: 0.323987] [G loss: 3.146493]\n",
      "[Epoch 4/200] [Batch 648/938] [D loss: 0.308600] [G loss: 1.853544]\n",
      "[Epoch 5/200] [Batch 110/938] [D loss: 0.309909] [G loss: 1.326990]\n",
      "[Epoch 5/200] [Batch 510/938] [D loss: 0.235691] [G loss: 3.604652]\n",
      "[Epoch 5/200] [Batch 910/938] [D loss: 0.380048] [G loss: 3.296381]\n",
      "[Epoch 6/200] [Batch 372/938] [D loss: 0.231563] [G loss: 1.665477]\n",
      "[Epoch 6/200] [Batch 772/938] [D loss: 0.536484] [G loss: 0.580105]\n",
      "[Epoch 7/200] [Batch 234/938] [D loss: 0.314912] [G loss: 1.323107]\n",
      "[Epoch 7/200] [Batch 634/938] [D loss: 0.281714] [G loss: 1.996446]\n",
      "[Epoch 8/200] [Batch 96/938] [D loss: 0.251878] [G loss: 1.433037]\n",
      "[Epoch 8/200] [Batch 496/938] [D loss: 0.333073] [G loss: 1.025449]\n",
      "[Epoch 8/200] [Batch 896/938] [D loss: 0.371112] [G loss: 1.041427]\n",
      "[Epoch 9/200] [Batch 358/938] [D loss: 0.184120] [G loss: 2.269948]\n",
      "[Epoch 9/200] [Batch 758/938] [D loss: 0.342882] [G loss: 1.041207]\n",
      "[Epoch 10/200] [Batch 220/938] [D loss: 0.183482] [G loss: 2.164997]\n",
      "[Epoch 10/200] [Batch 620/938] [D loss: 0.245855] [G loss: 1.328429]\n",
      "[Epoch 11/200] [Batch 82/938] [D loss: 0.506515] [G loss: 5.142179]\n",
      "[Epoch 11/200] [Batch 482/938] [D loss: 0.367878] [G loss: 3.628132]\n",
      "[Epoch 11/200] [Batch 882/938] [D loss: 0.159560] [G loss: 2.853934]\n",
      "[Epoch 12/200] [Batch 344/938] [D loss: 0.326093] [G loss: 1.281298]\n",
      "[Epoch 12/200] [Batch 744/938] [D loss: 0.336802] [G loss: 1.275815]\n",
      "[Epoch 13/200] [Batch 206/938] [D loss: 0.439142] [G loss: 1.107038]\n",
      "[Epoch 13/200] [Batch 606/938] [D loss: 0.295489] [G loss: 1.448064]\n",
      "[Epoch 14/200] [Batch 68/938] [D loss: 0.293027] [G loss: 1.650255]\n",
      "[Epoch 14/200] [Batch 468/938] [D loss: 0.310204] [G loss: 2.141643]\n",
      "[Epoch 14/200] [Batch 868/938] [D loss: 0.352867] [G loss: 1.433954]\n",
      "[Epoch 15/200] [Batch 330/938] [D loss: 0.290649] [G loss: 1.453035]\n",
      "[Epoch 15/200] [Batch 730/938] [D loss: 0.461105] [G loss: 0.806795]\n",
      "[Epoch 16/200] [Batch 192/938] [D loss: 0.275877] [G loss: 2.308867]\n",
      "[Epoch 16/200] [Batch 592/938] [D loss: 0.470954] [G loss: 0.806260]\n",
      "[Epoch 17/200] [Batch 54/938] [D loss: 0.313423] [G loss: 1.428701]\n",
      "[Epoch 17/200] [Batch 454/938] [D loss: 0.354455] [G loss: 2.718696]\n",
      "[Epoch 17/200] [Batch 854/938] [D loss: 0.385356] [G loss: 1.197387]\n",
      "[Epoch 18/200] [Batch 316/938] [D loss: 0.376740] [G loss: 1.224998]\n",
      "[Epoch 18/200] [Batch 716/938] [D loss: 0.335531] [G loss: 2.340378]\n",
      "[Epoch 19/200] [Batch 178/938] [D loss: 0.457560] [G loss: 2.337446]\n",
      "[Epoch 19/200] [Batch 578/938] [D loss: 0.314672] [G loss: 1.373808]\n",
      "[Epoch 20/200] [Batch 40/938] [D loss: 0.284311] [G loss: 2.332912]\n",
      "[Epoch 20/200] [Batch 440/938] [D loss: 0.299535] [G loss: 1.930819]\n",
      "[Epoch 20/200] [Batch 840/938] [D loss: 0.499050] [G loss: 3.054556]\n",
      "[Epoch 21/200] [Batch 302/938] [D loss: 0.344688] [G loss: 1.886921]\n",
      "[Epoch 21/200] [Batch 702/938] [D loss: 0.424420] [G loss: 1.718949]\n",
      "[Epoch 22/200] [Batch 164/938] [D loss: 0.351290] [G loss: 1.390171]\n",
      "[Epoch 22/200] [Batch 564/938] [D loss: 0.434796] [G loss: 2.254777]\n",
      "[Epoch 23/200] [Batch 26/938] [D loss: 0.329192] [G loss: 1.402748]\n",
      "[Epoch 23/200] [Batch 426/938] [D loss: 0.371851] [G loss: 2.002749]\n",
      "[Epoch 23/200] [Batch 826/938] [D loss: 0.327099] [G loss: 1.750334]\n",
      "[Epoch 24/200] [Batch 288/938] [D loss: 0.366553] [G loss: 2.683079]\n",
      "[Epoch 24/200] [Batch 688/938] [D loss: 0.517254] [G loss: 2.623801]\n",
      "[Epoch 25/200] [Batch 150/938] [D loss: 0.668852] [G loss: 0.592524]\n",
      "[Epoch 25/200] [Batch 550/938] [D loss: 0.399691] [G loss: 2.095175]\n",
      "[Epoch 26/200] [Batch 12/938] [D loss: 0.330709] [G loss: 2.045813]\n",
      "[Epoch 26/200] [Batch 412/938] [D loss: 0.299935] [G loss: 1.685052]\n",
      "[Epoch 26/200] [Batch 812/938] [D loss: 0.462047] [G loss: 0.943213]\n",
      "[Epoch 27/200] [Batch 274/938] [D loss: 0.467297] [G loss: 1.956659]\n",
      "[Epoch 27/200] [Batch 674/938] [D loss: 0.387670] [G loss: 1.616849]\n",
      "[Epoch 28/200] [Batch 136/938] [D loss: 0.430594] [G loss: 1.154207]\n",
      "[Epoch 28/200] [Batch 536/938] [D loss: 0.497853] [G loss: 0.697035]\n",
      "[Epoch 28/200] [Batch 936/938] [D loss: 0.369271] [G loss: 1.247672]\n",
      "[Epoch 29/200] [Batch 398/938] [D loss: 0.383592] [G loss: 1.653444]\n",
      "[Epoch 29/200] [Batch 798/938] [D loss: 0.433542] [G loss: 2.266972]\n",
      "[Epoch 30/200] [Batch 260/938] [D loss: 0.394854] [G loss: 1.390114]\n",
      "[Epoch 30/200] [Batch 660/938] [D loss: 0.445728] [G loss: 1.310950]\n",
      "[Epoch 31/200] [Batch 122/938] [D loss: 0.424404] [G loss: 1.578263]\n",
      "[Epoch 31/200] [Batch 522/938] [D loss: 0.413528] [G loss: 1.155913]\n",
      "[Epoch 31/200] [Batch 922/938] [D loss: 0.337695] [G loss: 1.731925]\n",
      "[Epoch 32/200] [Batch 384/938] [D loss: 0.315068] [G loss: 1.761425]\n",
      "[Epoch 32/200] [Batch 784/938] [D loss: 0.420037] [G loss: 1.511628]\n",
      "[Epoch 33/200] [Batch 246/938] [D loss: 0.423459] [G loss: 2.441308]\n",
      "[Epoch 33/200] [Batch 646/938] [D loss: 0.307800] [G loss: 1.727011]\n",
      "[Epoch 34/200] [Batch 108/938] [D loss: 0.434709] [G loss: 1.233444]\n",
      "[Epoch 34/200] [Batch 508/938] [D loss: 0.465503] [G loss: 0.804718]\n",
      "[Epoch 34/200] [Batch 908/938] [D loss: 0.370740] [G loss: 1.815086]\n",
      "[Epoch 35/200] [Batch 370/938] [D loss: 0.381959] [G loss: 1.612281]\n",
      "[Epoch 35/200] [Batch 770/938] [D loss: 0.342913] [G loss: 2.296454]\n",
      "[Epoch 36/200] [Batch 232/938] [D loss: 0.458834] [G loss: 1.779874]\n",
      "[Epoch 36/200] [Batch 632/938] [D loss: 0.377474] [G loss: 1.423216]\n",
      "[Epoch 37/200] [Batch 94/938] [D loss: 0.418347] [G loss: 1.560194]\n",
      "[Epoch 37/200] [Batch 494/938] [D loss: 0.270471] [G loss: 2.324134]\n",
      "[Epoch 37/200] [Batch 894/938] [D loss: 0.426987] [G loss: 1.427143]\n",
      "[Epoch 38/200] [Batch 356/938] [D loss: 0.393139] [G loss: 1.151186]\n",
      "[Epoch 38/200] [Batch 756/938] [D loss: 0.394304] [G loss: 1.417647]\n",
      "[Epoch 39/200] [Batch 218/938] [D loss: 0.440301] [G loss: 1.041059]\n",
      "[Epoch 39/200] [Batch 618/938] [D loss: 0.464796] [G loss: 2.533761]\n",
      "[Epoch 40/200] [Batch 80/938] [D loss: 0.353671] [G loss: 1.442198]\n",
      "[Epoch 40/200] [Batch 480/938] [D loss: 0.491123] [G loss: 2.002150]\n",
      "[Epoch 40/200] [Batch 880/938] [D loss: 0.447195] [G loss: 1.379069]\n",
      "[Epoch 41/200] [Batch 342/938] [D loss: 0.358615] [G loss: 1.703674]\n",
      "[Epoch 41/200] [Batch 742/938] [D loss: 0.380837] [G loss: 1.347118]\n",
      "[Epoch 42/200] [Batch 204/938] [D loss: 0.381479] [G loss: 1.780924]\n",
      "[Epoch 42/200] [Batch 604/938] [D loss: 0.296083] [G loss: 1.575087]\n",
      "[Epoch 43/200] [Batch 66/938] [D loss: 0.341997] [G loss: 1.823481]\n",
      "[Epoch 43/200] [Batch 466/938] [D loss: 0.540519] [G loss: 2.443894]\n",
      "[Epoch 43/200] [Batch 866/938] [D loss: 0.487074] [G loss: 1.410411]\n",
      "[Epoch 44/200] [Batch 328/938] [D loss: 0.378162] [G loss: 1.523007]\n",
      "[Epoch 44/200] [Batch 728/938] [D loss: 0.361438] [G loss: 1.758952]\n",
      "[Epoch 45/200] [Batch 190/938] [D loss: 0.354066] [G loss: 1.509583]\n",
      "[Epoch 45/200] [Batch 590/938] [D loss: 0.368072] [G loss: 1.822643]\n",
      "[Epoch 46/200] [Batch 52/938] [D loss: 0.402500] [G loss: 1.511656]\n",
      "[Epoch 46/200] [Batch 452/938] [D loss: 0.414056] [G loss: 1.740596]\n",
      "[Epoch 46/200] [Batch 852/938] [D loss: 0.377411] [G loss: 1.743928]\n",
      "[Epoch 47/200] [Batch 314/938] [D loss: 0.383963] [G loss: 1.201120]\n",
      "[Epoch 47/200] [Batch 714/938] [D loss: 0.377017] [G loss: 1.281142]\n",
      "[Epoch 48/200] [Batch 176/938] [D loss: 0.486425] [G loss: 2.376378]\n",
      "[Epoch 48/200] [Batch 576/938] [D loss: 0.389444] [G loss: 1.536267]\n",
      "[Epoch 49/200] [Batch 38/938] [D loss: 0.319202] [G loss: 1.828126]\n",
      "[Epoch 49/200] [Batch 438/938] [D loss: 0.338083] [G loss: 1.348922]\n",
      "[Epoch 49/200] [Batch 838/938] [D loss: 0.362826] [G loss: 1.576270]\n",
      "[Epoch 50/200] [Batch 300/938] [D loss: 0.419761] [G loss: 1.195447]\n",
      "[Epoch 50/200] [Batch 700/938] [D loss: 0.306653] [G loss: 2.245919]\n",
      "[Epoch 51/200] [Batch 162/938] [D loss: 0.432711] [G loss: 1.627598]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 51/200] [Batch 562/938] [D loss: 0.422541] [G loss: 1.475723]\n",
      "[Epoch 52/200] [Batch 24/938] [D loss: 0.333872] [G loss: 1.893930]\n",
      "[Epoch 52/200] [Batch 424/938] [D loss: 0.356275] [G loss: 2.178028]\n",
      "[Epoch 52/200] [Batch 824/938] [D loss: 0.487356] [G loss: 1.890546]\n",
      "[Epoch 53/200] [Batch 286/938] [D loss: 0.377878] [G loss: 1.881037]\n",
      "[Epoch 53/200] [Batch 686/938] [D loss: 0.432842] [G loss: 1.327764]\n",
      "[Epoch 54/200] [Batch 148/938] [D loss: 0.324685] [G loss: 1.905217]\n",
      "[Epoch 54/200] [Batch 548/938] [D loss: 0.358527] [G loss: 1.368164]\n",
      "[Epoch 55/200] [Batch 10/938] [D loss: 0.352783] [G loss: 1.695581]\n",
      "[Epoch 55/200] [Batch 410/938] [D loss: 0.438742] [G loss: 1.783901]\n",
      "[Epoch 55/200] [Batch 810/938] [D loss: 0.255529] [G loss: 1.977053]\n",
      "[Epoch 56/200] [Batch 272/938] [D loss: 0.341433] [G loss: 2.181850]\n",
      "[Epoch 56/200] [Batch 672/938] [D loss: 0.437403] [G loss: 1.547978]\n",
      "[Epoch 57/200] [Batch 134/938] [D loss: 0.376669] [G loss: 1.565433]\n",
      "[Epoch 57/200] [Batch 534/938] [D loss: 0.362623] [G loss: 1.534357]\n",
      "[Epoch 57/200] [Batch 934/938] [D loss: 0.422667] [G loss: 1.643914]\n",
      "[Epoch 58/200] [Batch 396/938] [D loss: 0.305147] [G loss: 1.828064]\n",
      "[Epoch 58/200] [Batch 796/938] [D loss: 0.412810] [G loss: 1.138774]\n",
      "[Epoch 59/200] [Batch 258/938] [D loss: 0.309188] [G loss: 1.500870]\n",
      "[Epoch 59/200] [Batch 658/938] [D loss: 0.426840] [G loss: 1.288126]\n",
      "[Epoch 60/200] [Batch 120/938] [D loss: 0.402245] [G loss: 2.174039]\n",
      "[Epoch 60/200] [Batch 520/938] [D loss: 0.377697] [G loss: 1.299198]\n",
      "[Epoch 60/200] [Batch 920/938] [D loss: 0.453773] [G loss: 1.023846]\n",
      "[Epoch 61/200] [Batch 382/938] [D loss: 0.370182] [G loss: 2.298614]\n",
      "[Epoch 61/200] [Batch 782/938] [D loss: 0.349529] [G loss: 1.528002]\n",
      "[Epoch 62/200] [Batch 244/938] [D loss: 0.361822] [G loss: 1.973861]\n",
      "[Epoch 62/200] [Batch 644/938] [D loss: 0.372537] [G loss: 2.249459]\n",
      "[Epoch 63/200] [Batch 106/938] [D loss: 0.465530] [G loss: 1.099658]\n",
      "[Epoch 63/200] [Batch 506/938] [D loss: 0.432606] [G loss: 1.054917]\n",
      "[Epoch 63/200] [Batch 906/938] [D loss: 0.317185] [G loss: 1.646805]\n",
      "[Epoch 64/200] [Batch 368/938] [D loss: 0.366755] [G loss: 1.650834]\n",
      "[Epoch 64/200] [Batch 768/938] [D loss: 0.382958] [G loss: 2.239507]\n",
      "[Epoch 65/200] [Batch 230/938] [D loss: 0.350756] [G loss: 2.294400]\n",
      "[Epoch 65/200] [Batch 630/938] [D loss: 0.463906] [G loss: 1.991433]\n",
      "[Epoch 66/200] [Batch 92/938] [D loss: 0.356845] [G loss: 1.687698]\n",
      "[Epoch 66/200] [Batch 492/938] [D loss: 0.439610] [G loss: 1.639666]\n",
      "[Epoch 66/200] [Batch 892/938] [D loss: 0.328125] [G loss: 1.564200]\n",
      "[Epoch 67/200] [Batch 354/938] [D loss: 0.454969] [G loss: 1.488466]\n",
      "[Epoch 67/200] [Batch 754/938] [D loss: 0.345227] [G loss: 2.124134]\n",
      "[Epoch 68/200] [Batch 216/938] [D loss: 0.360473] [G loss: 2.037869]\n",
      "[Epoch 68/200] [Batch 616/938] [D loss: 0.362788] [G loss: 2.228382]\n",
      "[Epoch 69/200] [Batch 78/938] [D loss: 0.339287] [G loss: 1.858522]\n",
      "[Epoch 69/200] [Batch 478/938] [D loss: 0.319205] [G loss: 1.780751]\n",
      "[Epoch 69/200] [Batch 878/938] [D loss: 0.381699] [G loss: 1.962058]\n",
      "[Epoch 70/200] [Batch 340/938] [D loss: 0.314691] [G loss: 2.231929]\n",
      "[Epoch 70/200] [Batch 740/938] [D loss: 0.424114] [G loss: 1.107577]\n",
      "[Epoch 71/200] [Batch 202/938] [D loss: 0.424182] [G loss: 1.819044]\n",
      "[Epoch 71/200] [Batch 602/938] [D loss: 0.346785] [G loss: 1.646202]\n",
      "[Epoch 72/200] [Batch 64/938] [D loss: 0.393883] [G loss: 2.031565]\n",
      "[Epoch 72/200] [Batch 464/938] [D loss: 0.440041] [G loss: 1.349225]\n",
      "[Epoch 72/200] [Batch 864/938] [D loss: 0.419128] [G loss: 1.894002]\n",
      "[Epoch 73/200] [Batch 326/938] [D loss: 0.339523] [G loss: 2.021562]\n",
      "[Epoch 73/200] [Batch 726/938] [D loss: 0.408970] [G loss: 1.715995]\n",
      "[Epoch 74/200] [Batch 188/938] [D loss: 0.431567] [G loss: 1.826395]\n",
      "[Epoch 74/200] [Batch 588/938] [D loss: 0.348818] [G loss: 1.490704]\n",
      "[Epoch 75/200] [Batch 50/938] [D loss: 0.402268] [G loss: 1.516497]\n",
      "[Epoch 75/200] [Batch 450/938] [D loss: 0.392569] [G loss: 1.529352]\n",
      "[Epoch 75/200] [Batch 850/938] [D loss: 0.247281] [G loss: 2.185824]\n",
      "[Epoch 76/200] [Batch 312/938] [D loss: 0.345226] [G loss: 1.574845]\n",
      "[Epoch 76/200] [Batch 712/938] [D loss: 0.447480] [G loss: 1.857559]\n",
      "[Epoch 77/200] [Batch 174/938] [D loss: 0.315650] [G loss: 1.922290]\n",
      "[Epoch 77/200] [Batch 574/938] [D loss: 0.315130] [G loss: 1.593383]\n",
      "[Epoch 78/200] [Batch 36/938] [D loss: 0.268268] [G loss: 1.525227]\n",
      "[Epoch 78/200] [Batch 436/938] [D loss: 0.350885] [G loss: 1.572384]\n",
      "[Epoch 78/200] [Batch 836/938] [D loss: 0.366905] [G loss: 1.752854]\n",
      "[Epoch 79/200] [Batch 298/938] [D loss: 0.380998] [G loss: 1.707451]\n",
      "[Epoch 79/200] [Batch 698/938] [D loss: 0.508511] [G loss: 2.039114]\n",
      "[Epoch 80/200] [Batch 160/938] [D loss: 0.468417] [G loss: 1.698304]\n",
      "[Epoch 80/200] [Batch 560/938] [D loss: 0.310145] [G loss: 1.929213]\n",
      "[Epoch 81/200] [Batch 22/938] [D loss: 0.391001] [G loss: 1.378347]\n",
      "[Epoch 81/200] [Batch 422/938] [D loss: 0.402386] [G loss: 1.766436]\n",
      "[Epoch 81/200] [Batch 822/938] [D loss: 0.444780] [G loss: 1.108675]\n",
      "[Epoch 82/200] [Batch 284/938] [D loss: 0.362758] [G loss: 1.400968]\n",
      "[Epoch 82/200] [Batch 684/938] [D loss: 0.331797] [G loss: 1.544860]\n",
      "[Epoch 83/200] [Batch 146/938] [D loss: 0.366319] [G loss: 2.061499]\n",
      "[Epoch 83/200] [Batch 546/938] [D loss: 0.481302] [G loss: 1.843165]\n",
      "[Epoch 84/200] [Batch 8/938] [D loss: 0.360507] [G loss: 1.516350]\n",
      "[Epoch 84/200] [Batch 408/938] [D loss: 0.324253] [G loss: 1.560611]\n",
      "[Epoch 84/200] [Batch 808/938] [D loss: 0.370660] [G loss: 1.649879]\n",
      "[Epoch 85/200] [Batch 270/938] [D loss: 0.372891] [G loss: 2.089428]\n",
      "[Epoch 85/200] [Batch 670/938] [D loss: 0.442426] [G loss: 1.951365]\n",
      "[Epoch 86/200] [Batch 132/938] [D loss: 0.334366] [G loss: 1.698368]\n",
      "[Epoch 86/200] [Batch 532/938] [D loss: 0.320342] [G loss: 1.662660]\n",
      "[Epoch 86/200] [Batch 932/938] [D loss: 0.415851] [G loss: 1.209113]\n",
      "[Epoch 87/200] [Batch 394/938] [D loss: 0.290885] [G loss: 1.499118]\n",
      "[Epoch 87/200] [Batch 794/938] [D loss: 0.394275] [G loss: 1.781033]\n",
      "[Epoch 88/200] [Batch 256/938] [D loss: 0.293459] [G loss: 2.815826]\n",
      "[Epoch 88/200] [Batch 656/938] [D loss: 0.394237] [G loss: 1.668519]\n",
      "[Epoch 89/200] [Batch 118/938] [D loss: 0.477553] [G loss: 1.697932]\n",
      "[Epoch 89/200] [Batch 518/938] [D loss: 0.356975] [G loss: 1.826853]\n",
      "[Epoch 89/200] [Batch 918/938] [D loss: 0.354465] [G loss: 1.802291]\n",
      "[Epoch 90/200] [Batch 380/938] [D loss: 0.337215] [G loss: 1.863813]\n",
      "[Epoch 90/200] [Batch 780/938] [D loss: 0.394425] [G loss: 1.845012]\n",
      "[Epoch 91/200] [Batch 242/938] [D loss: 0.461499] [G loss: 1.227898]\n",
      "[Epoch 91/200] [Batch 642/938] [D loss: 0.389785] [G loss: 1.350511]\n",
      "[Epoch 92/200] [Batch 104/938] [D loss: 0.428652] [G loss: 1.224218]\n",
      "[Epoch 92/200] [Batch 504/938] [D loss: 0.347021] [G loss: 1.589660]\n",
      "[Epoch 92/200] [Batch 904/938] [D loss: 0.387793] [G loss: 1.567064]\n",
      "[Epoch 93/200] [Batch 366/938] [D loss: 0.379838] [G loss: 1.663946]\n",
      "[Epoch 93/200] [Batch 766/938] [D loss: 0.399983] [G loss: 1.633635]\n",
      "[Epoch 94/200] [Batch 228/938] [D loss: 0.352005] [G loss: 1.464773]\n",
      "[Epoch 94/200] [Batch 628/938] [D loss: 0.433244] [G loss: 1.862936]\n",
      "[Epoch 95/200] [Batch 90/938] [D loss: 0.389238] [G loss: 1.946919]\n",
      "[Epoch 95/200] [Batch 490/938] [D loss: 0.328519] [G loss: 1.776858]\n",
      "[Epoch 95/200] [Batch 890/938] [D loss: 0.443824] [G loss: 1.668122]\n",
      "[Epoch 96/200] [Batch 352/938] [D loss: 0.471000] [G loss: 1.730378]\n",
      "[Epoch 96/200] [Batch 752/938] [D loss: 0.370076] [G loss: 1.477778]\n",
      "[Epoch 97/200] [Batch 214/938] [D loss: 0.417451] [G loss: 1.726684]\n",
      "[Epoch 97/200] [Batch 614/938] [D loss: 0.305685] [G loss: 1.442706]\n",
      "[Epoch 98/200] [Batch 76/938] [D loss: 0.412254] [G loss: 1.965675]\n",
      "[Epoch 98/200] [Batch 476/938] [D loss: 0.359150] [G loss: 1.744383]\n",
      "[Epoch 98/200] [Batch 876/938] [D loss: 0.368210] [G loss: 1.655138]\n",
      "[Epoch 99/200] [Batch 338/938] [D loss: 0.473993] [G loss: 1.730807]\n",
      "[Epoch 99/200] [Batch 738/938] [D loss: 0.424877] [G loss: 2.072725]\n",
      "[Epoch 100/200] [Batch 200/938] [D loss: 0.427471] [G loss: 1.384975]\n",
      "[Epoch 100/200] [Batch 600/938] [D loss: 0.467293] [G loss: 1.821262]\n",
      "[Epoch 101/200] [Batch 62/938] [D loss: 0.371946] [G loss: 1.737914]\n",
      "[Epoch 101/200] [Batch 462/938] [D loss: 0.426444] [G loss: 1.780433]\n",
      "[Epoch 101/200] [Batch 862/938] [D loss: 0.392855] [G loss: 1.357322]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 102/200] [Batch 324/938] [D loss: 0.405372] [G loss: 1.351003]\n",
      "[Epoch 102/200] [Batch 724/938] [D loss: 0.411918] [G loss: 1.354909]\n",
      "[Epoch 103/200] [Batch 186/938] [D loss: 0.461757] [G loss: 1.696794]\n",
      "[Epoch 103/200] [Batch 586/938] [D loss: 0.328002] [G loss: 1.896736]\n",
      "[Epoch 104/200] [Batch 48/938] [D loss: 0.444186] [G loss: 2.072689]\n",
      "[Epoch 104/200] [Batch 448/938] [D loss: 0.438934] [G loss: 1.736473]\n",
      "[Epoch 104/200] [Batch 848/938] [D loss: 0.364862] [G loss: 1.614417]\n",
      "[Epoch 105/200] [Batch 310/938] [D loss: 0.356435] [G loss: 2.200775]\n",
      "[Epoch 105/200] [Batch 710/938] [D loss: 0.422420] [G loss: 1.391967]\n",
      "[Epoch 106/200] [Batch 172/938] [D loss: 0.422086] [G loss: 2.312851]\n",
      "[Epoch 106/200] [Batch 572/938] [D loss: 0.398597] [G loss: 2.369773]\n",
      "[Epoch 107/200] [Batch 34/938] [D loss: 0.409924] [G loss: 1.922763]\n",
      "[Epoch 107/200] [Batch 434/938] [D loss: 0.414433] [G loss: 1.816016]\n",
      "[Epoch 107/200] [Batch 834/938] [D loss: 0.354329] [G loss: 1.474966]\n",
      "[Epoch 108/200] [Batch 296/938] [D loss: 0.398364] [G loss: 2.061161]\n",
      "[Epoch 108/200] [Batch 696/938] [D loss: 0.336787] [G loss: 1.661459]\n",
      "[Epoch 109/200] [Batch 158/938] [D loss: 0.351367] [G loss: 1.772548]\n",
      "[Epoch 109/200] [Batch 558/938] [D loss: 0.389760] [G loss: 1.775961]\n",
      "[Epoch 110/200] [Batch 20/938] [D loss: 0.444482] [G loss: 1.811221]\n",
      "[Epoch 110/200] [Batch 420/938] [D loss: 0.320367] [G loss: 1.837831]\n",
      "[Epoch 110/200] [Batch 820/938] [D loss: 0.517823] [G loss: 1.899471]\n",
      "[Epoch 111/200] [Batch 282/938] [D loss: 0.389110] [G loss: 1.564435]\n",
      "[Epoch 111/200] [Batch 682/938] [D loss: 0.402767] [G loss: 1.775751]\n",
      "[Epoch 112/200] [Batch 144/938] [D loss: 0.377371] [G loss: 2.139977]\n",
      "[Epoch 112/200] [Batch 544/938] [D loss: 0.359860] [G loss: 1.311604]\n",
      "[Epoch 113/200] [Batch 6/938] [D loss: 0.336670] [G loss: 1.478230]\n",
      "[Epoch 113/200] [Batch 406/938] [D loss: 0.387754] [G loss: 1.743085]\n",
      "[Epoch 113/200] [Batch 806/938] [D loss: 0.373083] [G loss: 1.470573]\n",
      "[Epoch 114/200] [Batch 268/938] [D loss: 0.480918] [G loss: 1.580396]\n",
      "[Epoch 114/200] [Batch 668/938] [D loss: 0.522246] [G loss: 1.929161]\n",
      "[Epoch 115/200] [Batch 130/938] [D loss: 0.408328] [G loss: 1.761633]\n",
      "[Epoch 115/200] [Batch 530/938] [D loss: 0.377816] [G loss: 1.831297]\n",
      "[Epoch 115/200] [Batch 930/938] [D loss: 0.305928] [G loss: 2.056948]\n",
      "[Epoch 116/200] [Batch 392/938] [D loss: 0.420066] [G loss: 1.477497]\n",
      "[Epoch 116/200] [Batch 792/938] [D loss: 0.301346] [G loss: 1.489162]\n",
      "[Epoch 117/200] [Batch 254/938] [D loss: 0.307509] [G loss: 1.485340]\n",
      "[Epoch 117/200] [Batch 654/938] [D loss: 0.448183] [G loss: 1.901285]\n",
      "[Epoch 118/200] [Batch 116/938] [D loss: 0.318986] [G loss: 1.621984]\n",
      "[Epoch 118/200] [Batch 516/938] [D loss: 0.495243] [G loss: 1.060807]\n",
      "[Epoch 118/200] [Batch 916/938] [D loss: 0.404760] [G loss: 1.971943]\n",
      "[Epoch 119/200] [Batch 378/938] [D loss: 0.379871] [G loss: 1.893677]\n",
      "[Epoch 119/200] [Batch 778/938] [D loss: 0.341047] [G loss: 2.086266]\n",
      "[Epoch 120/200] [Batch 240/938] [D loss: 0.415624] [G loss: 1.484673]\n",
      "[Epoch 120/200] [Batch 640/938] [D loss: 0.381553] [G loss: 1.972584]\n",
      "[Epoch 121/200] [Batch 102/938] [D loss: 0.392135] [G loss: 1.966844]\n",
      "[Epoch 121/200] [Batch 502/938] [D loss: 0.412248] [G loss: 1.214909]\n",
      "[Epoch 121/200] [Batch 902/938] [D loss: 0.462889] [G loss: 1.972207]\n",
      "[Epoch 122/200] [Batch 364/938] [D loss: 0.300566] [G loss: 1.673832]\n",
      "[Epoch 122/200] [Batch 764/938] [D loss: 0.370215] [G loss: 1.935500]\n",
      "[Epoch 123/200] [Batch 226/938] [D loss: 0.512846] [G loss: 1.744505]\n",
      "[Epoch 123/200] [Batch 626/938] [D loss: 0.376373] [G loss: 1.341427]\n",
      "[Epoch 124/200] [Batch 88/938] [D loss: 0.371965] [G loss: 1.640301]\n",
      "[Epoch 124/200] [Batch 488/938] [D loss: 0.397178] [G loss: 1.596113]\n",
      "[Epoch 124/200] [Batch 888/938] [D loss: 0.394394] [G loss: 1.713914]\n",
      "[Epoch 125/200] [Batch 350/938] [D loss: 0.442999] [G loss: 1.594931]\n",
      "[Epoch 125/200] [Batch 750/938] [D loss: 0.435590] [G loss: 1.404670]\n",
      "[Epoch 126/200] [Batch 212/938] [D loss: 0.384255] [G loss: 1.795815]\n",
      "[Epoch 126/200] [Batch 612/938] [D loss: 0.358449] [G loss: 1.678500]\n",
      "[Epoch 127/200] [Batch 74/938] [D loss: 0.338400] [G loss: 1.631910]\n",
      "[Epoch 127/200] [Batch 474/938] [D loss: 0.426662] [G loss: 1.807815]\n",
      "[Epoch 127/200] [Batch 874/938] [D loss: 0.425915] [G loss: 1.412254]\n",
      "[Epoch 128/200] [Batch 336/938] [D loss: 0.385778] [G loss: 1.628028]\n",
      "[Epoch 128/200] [Batch 736/938] [D loss: 0.449118] [G loss: 1.190480]\n",
      "[Epoch 129/200] [Batch 198/938] [D loss: 0.430083] [G loss: 1.352705]\n",
      "[Epoch 129/200] [Batch 598/938] [D loss: 0.473688] [G loss: 1.329874]\n",
      "[Epoch 130/200] [Batch 60/938] [D loss: 0.421912] [G loss: 1.688032]\n",
      "[Epoch 130/200] [Batch 460/938] [D loss: 0.473079] [G loss: 1.271689]\n",
      "[Epoch 130/200] [Batch 860/938] [D loss: 0.508267] [G loss: 1.152074]\n",
      "[Epoch 131/200] [Batch 322/938] [D loss: 0.404246] [G loss: 1.621921]\n",
      "[Epoch 131/200] [Batch 722/938] [D loss: 0.400539] [G loss: 1.755152]\n",
      "[Epoch 132/200] [Batch 184/938] [D loss: 0.367013] [G loss: 1.648282]\n",
      "[Epoch 132/200] [Batch 584/938] [D loss: 0.406021] [G loss: 1.670289]\n",
      "[Epoch 133/200] [Batch 46/938] [D loss: 0.416846] [G loss: 1.645901]\n",
      "[Epoch 133/200] [Batch 446/938] [D loss: 0.367809] [G loss: 1.620158]\n",
      "[Epoch 133/200] [Batch 846/938] [D loss: 0.371297] [G loss: 1.772114]\n",
      "[Epoch 134/200] [Batch 308/938] [D loss: 0.357048] [G loss: 1.442061]\n",
      "[Epoch 134/200] [Batch 708/938] [D loss: 0.429268] [G loss: 2.340898]\n",
      "[Epoch 135/200] [Batch 170/938] [D loss: 0.376399] [G loss: 1.838800]\n",
      "[Epoch 135/200] [Batch 570/938] [D loss: 0.369888] [G loss: 1.786632]\n",
      "[Epoch 136/200] [Batch 32/938] [D loss: 0.399686] [G loss: 1.721207]\n",
      "[Epoch 136/200] [Batch 432/938] [D loss: 0.443929] [G loss: 1.838277]\n",
      "[Epoch 136/200] [Batch 832/938] [D loss: 0.419119] [G loss: 1.779048]\n",
      "[Epoch 137/200] [Batch 294/938] [D loss: 0.450915] [G loss: 1.178668]\n",
      "[Epoch 137/200] [Batch 694/938] [D loss: 0.337362] [G loss: 1.447456]\n",
      "[Epoch 138/200] [Batch 156/938] [D loss: 0.426239] [G loss: 1.094733]\n",
      "[Epoch 138/200] [Batch 556/938] [D loss: 0.444125] [G loss: 1.875298]\n",
      "[Epoch 139/200] [Batch 18/938] [D loss: 0.427919] [G loss: 1.843579]\n",
      "[Epoch 139/200] [Batch 418/938] [D loss: 0.369925] [G loss: 1.533062]\n",
      "[Epoch 139/200] [Batch 818/938] [D loss: 0.379154] [G loss: 1.254229]\n",
      "[Epoch 140/200] [Batch 280/938] [D loss: 0.411088] [G loss: 1.689849]\n",
      "[Epoch 140/200] [Batch 680/938] [D loss: 0.418363] [G loss: 1.990081]\n",
      "[Epoch 141/200] [Batch 142/938] [D loss: 0.343406] [G loss: 1.532863]\n",
      "[Epoch 141/200] [Batch 542/938] [D loss: 0.404001] [G loss: 1.878184]\n",
      "[Epoch 142/200] [Batch 4/938] [D loss: 0.477154] [G loss: 1.955855]\n",
      "[Epoch 142/200] [Batch 404/938] [D loss: 0.418584] [G loss: 1.597144]\n",
      "[Epoch 142/200] [Batch 804/938] [D loss: 0.428816] [G loss: 1.324190]\n",
      "[Epoch 143/200] [Batch 266/938] [D loss: 0.428795] [G loss: 1.667989]\n",
      "[Epoch 143/200] [Batch 666/938] [D loss: 0.431368] [G loss: 2.024033]\n",
      "[Epoch 144/200] [Batch 128/938] [D loss: 0.397834] [G loss: 1.871116]\n",
      "[Epoch 144/200] [Batch 528/938] [D loss: 0.488046] [G loss: 1.636125]\n",
      "[Epoch 144/200] [Batch 928/938] [D loss: 0.333028] [G loss: 2.091977]\n",
      "[Epoch 145/200] [Batch 390/938] [D loss: 0.395809] [G loss: 1.626601]\n",
      "[Epoch 145/200] [Batch 790/938] [D loss: 0.415861] [G loss: 1.666748]\n",
      "[Epoch 146/200] [Batch 252/938] [D loss: 0.383274] [G loss: 1.750764]\n",
      "[Epoch 146/200] [Batch 652/938] [D loss: 0.375954] [G loss: 1.989911]\n",
      "[Epoch 147/200] [Batch 114/938] [D loss: 0.411329] [G loss: 1.604100]\n",
      "[Epoch 147/200] [Batch 514/938] [D loss: 0.327078] [G loss: 1.449241]\n",
      "[Epoch 147/200] [Batch 914/938] [D loss: 0.424371] [G loss: 1.633216]\n",
      "[Epoch 148/200] [Batch 376/938] [D loss: 0.407801] [G loss: 1.313560]\n",
      "[Epoch 148/200] [Batch 776/938] [D loss: 0.381808] [G loss: 1.808215]\n",
      "[Epoch 149/200] [Batch 238/938] [D loss: 0.357106] [G loss: 1.622700]\n",
      "[Epoch 149/200] [Batch 638/938] [D loss: 0.351595] [G loss: 1.573914]\n",
      "[Epoch 150/200] [Batch 100/938] [D loss: 0.397871] [G loss: 1.685507]\n",
      "[Epoch 150/200] [Batch 500/938] [D loss: 0.375202] [G loss: 1.881801]\n",
      "[Epoch 150/200] [Batch 900/938] [D loss: 0.404706] [G loss: 1.649149]\n",
      "[Epoch 151/200] [Batch 362/938] [D loss: 0.314609] [G loss: 2.174711]\n",
      "[Epoch 151/200] [Batch 762/938] [D loss: 0.341314] [G loss: 1.587099]\n",
      "[Epoch 152/200] [Batch 224/938] [D loss: 0.475950] [G loss: 1.452308]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 152/200] [Batch 624/938] [D loss: 0.431338] [G loss: 1.332425]\n",
      "[Epoch 153/200] [Batch 86/938] [D loss: 0.411982] [G loss: 1.589581]\n",
      "[Epoch 153/200] [Batch 486/938] [D loss: 0.452257] [G loss: 2.041316]\n",
      "[Epoch 153/200] [Batch 886/938] [D loss: 0.429951] [G loss: 1.973903]\n",
      "[Epoch 154/200] [Batch 348/938] [D loss: 0.505281] [G loss: 1.622912]\n",
      "[Epoch 154/200] [Batch 748/938] [D loss: 0.426713] [G loss: 1.238048]\n",
      "[Epoch 155/200] [Batch 210/938] [D loss: 0.343675] [G loss: 1.652057]\n",
      "[Epoch 155/200] [Batch 610/938] [D loss: 0.468899] [G loss: 1.594764]\n",
      "[Epoch 156/200] [Batch 72/938] [D loss: 0.437651] [G loss: 1.926811]\n",
      "[Epoch 156/200] [Batch 472/938] [D loss: 0.344062] [G loss: 1.640561]\n",
      "[Epoch 156/200] [Batch 872/938] [D loss: 0.504861] [G loss: 1.521666]\n",
      "[Epoch 157/200] [Batch 334/938] [D loss: 0.432307] [G loss: 1.668160]\n",
      "[Epoch 157/200] [Batch 734/938] [D loss: 0.474866] [G loss: 1.387235]\n",
      "[Epoch 158/200] [Batch 196/938] [D loss: 0.417040] [G loss: 1.441964]\n",
      "[Epoch 158/200] [Batch 596/938] [D loss: 0.338227] [G loss: 1.691872]\n",
      "[Epoch 159/200] [Batch 58/938] [D loss: 0.403683] [G loss: 1.592554]\n",
      "[Epoch 159/200] [Batch 458/938] [D loss: 0.412506] [G loss: 1.262919]\n",
      "[Epoch 159/200] [Batch 858/938] [D loss: 0.502827] [G loss: 1.016959]\n",
      "[Epoch 160/200] [Batch 320/938] [D loss: 0.393061] [G loss: 1.670414]\n",
      "[Epoch 160/200] [Batch 720/938] [D loss: 0.430360] [G loss: 1.897630]\n",
      "[Epoch 161/200] [Batch 182/938] [D loss: 0.373214] [G loss: 1.518409]\n",
      "[Epoch 161/200] [Batch 582/938] [D loss: 0.419745] [G loss: 1.494798]\n",
      "[Epoch 162/200] [Batch 44/938] [D loss: 0.457836] [G loss: 1.531096]\n",
      "[Epoch 162/200] [Batch 444/938] [D loss: 0.371175] [G loss: 1.558800]\n",
      "[Epoch 162/200] [Batch 844/938] [D loss: 0.409257] [G loss: 1.913951]\n",
      "[Epoch 163/200] [Batch 306/938] [D loss: 0.565629] [G loss: 1.384099]\n",
      "[Epoch 163/200] [Batch 706/938] [D loss: 0.371327] [G loss: 1.508207]\n",
      "[Epoch 164/200] [Batch 168/938] [D loss: 0.373752] [G loss: 1.315325]\n",
      "[Epoch 164/200] [Batch 568/938] [D loss: 0.394537] [G loss: 1.964137]\n",
      "[Epoch 165/200] [Batch 30/938] [D loss: 0.385952] [G loss: 1.505888]\n",
      "[Epoch 165/200] [Batch 430/938] [D loss: 0.369851] [G loss: 1.659375]\n",
      "[Epoch 165/200] [Batch 830/938] [D loss: 0.418279] [G loss: 1.907534]\n",
      "[Epoch 166/200] [Batch 292/938] [D loss: 0.436873] [G loss: 1.709330]\n",
      "[Epoch 166/200] [Batch 692/938] [D loss: 0.332080] [G loss: 1.900820]\n",
      "[Epoch 167/200] [Batch 154/938] [D loss: 0.409774] [G loss: 2.258373]\n",
      "[Epoch 167/200] [Batch 554/938] [D loss: 0.479116] [G loss: 1.395512]\n",
      "[Epoch 168/200] [Batch 16/938] [D loss: 0.460653] [G loss: 1.737925]\n",
      "[Epoch 168/200] [Batch 416/938] [D loss: 0.365250] [G loss: 1.637163]\n",
      "[Epoch 168/200] [Batch 816/938] [D loss: 0.433363] [G loss: 1.262131]\n",
      "[Epoch 169/200] [Batch 278/938] [D loss: 0.362560] [G loss: 1.547716]\n",
      "[Epoch 169/200] [Batch 678/938] [D loss: 0.495629] [G loss: 1.620241]\n",
      "[Epoch 170/200] [Batch 140/938] [D loss: 0.405091] [G loss: 1.580024]\n",
      "[Epoch 170/200] [Batch 540/938] [D loss: 0.482487] [G loss: 1.296492]\n",
      "[Epoch 171/200] [Batch 2/938] [D loss: 0.381007] [G loss: 1.671313]\n",
      "[Epoch 171/200] [Batch 402/938] [D loss: 0.446138] [G loss: 1.255805]\n",
      "[Epoch 171/200] [Batch 802/938] [D loss: 0.457618] [G loss: 1.355641]\n",
      "[Epoch 172/200] [Batch 264/938] [D loss: 0.430833] [G loss: 1.398663]\n",
      "[Epoch 172/200] [Batch 664/938] [D loss: 0.405074] [G loss: 1.678625]\n",
      "[Epoch 173/200] [Batch 126/938] [D loss: 0.376223] [G loss: 1.825859]\n",
      "[Epoch 173/200] [Batch 526/938] [D loss: 0.448539] [G loss: 1.519432]\n",
      "[Epoch 173/200] [Batch 926/938] [D loss: 0.462150] [G loss: 1.338064]\n",
      "[Epoch 174/200] [Batch 388/938] [D loss: 0.409059] [G loss: 1.594913]\n",
      "[Epoch 174/200] [Batch 788/938] [D loss: 0.428834] [G loss: 1.343052]\n",
      "[Epoch 175/200] [Batch 250/938] [D loss: 0.419728] [G loss: 1.624729]\n",
      "[Epoch 175/200] [Batch 650/938] [D loss: 0.452529] [G loss: 2.039319]\n",
      "[Epoch 176/200] [Batch 112/938] [D loss: 0.454043] [G loss: 1.636027]\n",
      "[Epoch 176/200] [Batch 512/938] [D loss: 0.447894] [G loss: 1.581606]\n",
      "[Epoch 176/200] [Batch 912/938] [D loss: 0.398090] [G loss: 1.691530]\n",
      "[Epoch 177/200] [Batch 374/938] [D loss: 0.391714] [G loss: 1.473728]\n",
      "[Epoch 177/200] [Batch 774/938] [D loss: 0.354250] [G loss: 1.615680]\n",
      "[Epoch 178/200] [Batch 236/938] [D loss: 0.412373] [G loss: 1.753430]\n",
      "[Epoch 178/200] [Batch 636/938] [D loss: 0.357003] [G loss: 1.400923]\n",
      "[Epoch 179/200] [Batch 98/938] [D loss: 0.405538] [G loss: 1.459984]\n",
      "[Epoch 179/200] [Batch 498/938] [D loss: 0.423205] [G loss: 1.669837]\n",
      "[Epoch 179/200] [Batch 898/938] [D loss: 0.353473] [G loss: 1.693623]\n",
      "[Epoch 180/200] [Batch 360/938] [D loss: 0.426932] [G loss: 1.741618]\n",
      "[Epoch 180/200] [Batch 760/938] [D loss: 0.463907] [G loss: 1.273517]\n",
      "[Epoch 181/200] [Batch 222/938] [D loss: 0.422668] [G loss: 1.551256]\n",
      "[Epoch 181/200] [Batch 622/938] [D loss: 0.400081] [G loss: 1.670930]\n",
      "[Epoch 182/200] [Batch 84/938] [D loss: 0.378610] [G loss: 1.953127]\n",
      "[Epoch 182/200] [Batch 484/938] [D loss: 0.431687] [G loss: 1.300578]\n",
      "[Epoch 182/200] [Batch 884/938] [D loss: 0.387550] [G loss: 1.521659]\n",
      "[Epoch 183/200] [Batch 346/938] [D loss: 0.410756] [G loss: 1.489563]\n",
      "[Epoch 183/200] [Batch 746/938] [D loss: 0.466577] [G loss: 1.494755]\n",
      "[Epoch 184/200] [Batch 208/938] [D loss: 0.414447] [G loss: 1.595468]\n",
      "[Epoch 184/200] [Batch 608/938] [D loss: 0.417339] [G loss: 1.894751]\n",
      "[Epoch 185/200] [Batch 70/938] [D loss: 0.411940] [G loss: 1.407733]\n",
      "[Epoch 185/200] [Batch 470/938] [D loss: 0.472545] [G loss: 1.416830]\n",
      "[Epoch 185/200] [Batch 870/938] [D loss: 0.417601] [G loss: 1.685631]\n",
      "[Epoch 186/200] [Batch 332/938] [D loss: 0.404886] [G loss: 1.689413]\n",
      "[Epoch 186/200] [Batch 732/938] [D loss: 0.480243] [G loss: 1.932532]\n",
      "[Epoch 187/200] [Batch 194/938] [D loss: 0.354701] [G loss: 1.647193]\n",
      "[Epoch 187/200] [Batch 594/938] [D loss: 0.399001] [G loss: 1.689227]\n",
      "[Epoch 188/200] [Batch 56/938] [D loss: 0.410710] [G loss: 1.264424]\n",
      "[Epoch 188/200] [Batch 456/938] [D loss: 0.396948] [G loss: 1.730629]\n",
      "[Epoch 188/200] [Batch 856/938] [D loss: 0.345122] [G loss: 1.722534]\n",
      "[Epoch 189/200] [Batch 318/938] [D loss: 0.489451] [G loss: 1.482501]\n",
      "[Epoch 189/200] [Batch 718/938] [D loss: 0.437541] [G loss: 1.361133]\n",
      "[Epoch 190/200] [Batch 180/938] [D loss: 0.379701] [G loss: 1.890969]\n",
      "[Epoch 190/200] [Batch 580/938] [D loss: 0.392935] [G loss: 1.414068]\n",
      "[Epoch 191/200] [Batch 42/938] [D loss: 0.463644] [G loss: 1.390298]\n",
      "[Epoch 191/200] [Batch 442/938] [D loss: 0.408480] [G loss: 1.744873]\n",
      "[Epoch 191/200] [Batch 842/938] [D loss: 0.414307] [G loss: 1.232235]\n",
      "[Epoch 192/200] [Batch 304/938] [D loss: 0.364895] [G loss: 1.821955]\n",
      "[Epoch 192/200] [Batch 704/938] [D loss: 0.486766] [G loss: 1.739134]\n",
      "[Epoch 193/200] [Batch 166/938] [D loss: 0.444366] [G loss: 1.334910]\n",
      "[Epoch 193/200] [Batch 566/938] [D loss: 0.525060] [G loss: 1.554776]\n",
      "[Epoch 194/200] [Batch 28/938] [D loss: 0.526493] [G loss: 1.471936]\n",
      "[Epoch 194/200] [Batch 428/938] [D loss: 0.487691] [G loss: 1.586165]\n",
      "[Epoch 194/200] [Batch 828/938] [D loss: 0.488804] [G loss: 1.666324]\n",
      "[Epoch 195/200] [Batch 290/938] [D loss: 0.472743] [G loss: 1.910866]\n",
      "[Epoch 195/200] [Batch 690/938] [D loss: 0.411322] [G loss: 1.361565]\n",
      "[Epoch 196/200] [Batch 152/938] [D loss: 0.428913] [G loss: 1.647619]\n",
      "[Epoch 196/200] [Batch 552/938] [D loss: 0.418852] [G loss: 1.517789]\n",
      "[Epoch 197/200] [Batch 14/938] [D loss: 0.415958] [G loss: 1.692321]\n",
      "[Epoch 197/200] [Batch 414/938] [D loss: 0.440011] [G loss: 1.690017]\n",
      "[Epoch 197/200] [Batch 814/938] [D loss: 0.404821] [G loss: 1.732472]\n",
      "[Epoch 198/200] [Batch 276/938] [D loss: 0.449719] [G loss: 1.446860]\n",
      "[Epoch 198/200] [Batch 676/938] [D loss: 0.487123] [G loss: 1.517798]\n",
      "[Epoch 199/200] [Batch 138/938] [D loss: 0.472005] [G loss: 1.438426]\n",
      "[Epoch 199/200] [Batch 538/938] [D loss: 0.390182] [G loss: 1.651227]\n"
     ]
    }
   ],
   "source": [
    "# ----------\n",
    "#  Training\n",
    "# ----------\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(Tensor))\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        if batches_done % sample_interval == 0:# and batches_done != 0:\n",
    "            print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\" % (epoch, n_epochs, i, len(dataloader), d_loss.item(), g_loss.item()))\n",
    "            \n",
    "            batches_done_str = str(batches_done).zfill(8)\n",
    "            \n",
    "            # plot a grid of samples\n",
    "            x = gen_imgs.data[:,0,:,:].cpu()\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            plt.suptitle(\"Sampled Images, Batch_Num: \" + str(batches_done), fontsize=16)\n",
    "            for j in range(64):\n",
    "                plt.subplot(8, 8, j + 1)\n",
    "                plt.imshow(x[j].reshape([28,28]), cmap='gray')\n",
    "                plt.xticks(())\n",
    "                plt.yticks(())\n",
    "            plt.savefig('images/sample_images_' + batches_done_str + '.png')\n",
    "            plt.close()\n",
    "            #plt.show()\n",
    "\n",
    "            ## plot the 2d triangle distance density plot\n",
    "            Z = Variable(Tensor(np.random.normal(0, 1, (1000, latent_dim))))\n",
    "            gen_imgs = generator(Z)\n",
    "            X = gen_imgs.data[:,0,:,:].cpu().numpy()\n",
    "\n",
    "            [K_distances, K_angles] = triangle_distributions(X, 10000)\n",
    "            fig,ax = plt.subplots(1,1)\n",
    "            ax1 = sns.kdeplot(K_distances[:,0], K_distances[:,1])\n",
    "            ax1.set_xlim([0, 1.0])\n",
    "            ax1.set_ylim([0, 1.0])\n",
    "            ax1.set_xlabel(r\"$d_{mid}-d_{min}$\", fontsize=16)\n",
    "            ax1.set_ylabel(r\"$d_{max}-d_{mid}$\", fontsize=16)\n",
    "            ax1.set_title(\"Distance Density Plot, Batch_Num: \" + str(batches_done), fontsize=16)\n",
    "            plt.savefig('images/distance_plot_' + batches_done_str + '.png')\n",
    "            plt.close()\n",
    "            #plt.show()\n",
    "            \n",
    "            ## plot the 2d triangle angle density plot\n",
    "            fig,ax = plt.subplots(1,1)\n",
    "            ax2 = sns.kdeplot(K_angles[:,0], K_angles[:,1])\n",
    "            ax2.set_xlim([0, 1.0])\n",
    "            ax2.set_ylim([0, 1.0])\n",
    "            ax2.set_xlabel(r\"$\\theta_{min}/\\theta_{mid}$\", fontsize=16)\n",
    "            ax2.set_ylabel(r\"$\\theta_{min}/\\theta_{max}$\", fontsize=16)\n",
    "            ax2.set_title(\"Angle Density Plot, Batch_Num: \" + str(batches_done), fontsize=16)\n",
    "            plt.savefig('images/triangle_plot_' + batches_done_str + '.png')\n",
    "            plt.close()            \n",
    "            #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## look at 1d measures of UM\n",
    "## look at raw MNIST data\n",
    "## double check code\n",
    "## look into top-subspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
